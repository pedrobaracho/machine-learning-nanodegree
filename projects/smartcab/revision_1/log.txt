
/-------------------------
| Training trial 1
\-------------------------

Environment.reset(): Trial set up with start = (2, 2), destination = (7, 4), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.5227 < 0.9500
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 2.77563918852
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 2.7756391885187215, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.77563918852, Q[state]-after: {'forward': 0.0, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.6614 < 0.9500
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: left, reward: -10.00237504
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': -10.002375040023024, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -10.00237504, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -5.001187520011512}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.1392 < 0.9500
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: right, reward: 0.47146967213
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 0.47146967212988256, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -5.001187520011512}, action: right, reward: 0.47146967213, Q[state]-after: {'forward': 0.0, 'right': 0.23573483606494128, None: 0.0, 'left': -5.001187520011512}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.8587 < 0.9500
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: None, reward: 2.52482993799
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', 'forward', None), 'deadline': 22, 't': 3, 'action': None, 'reward': 2.524829937992796, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.52482993799, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.262414968996398, 'left': 0.0}
Agent previous state: ('left', 'red', 'right', 'forward', None)
Agent properly idled at a red light. (rewarded 2.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.2423 < 0.9500
Environment.act() [POST]: location: (1, 7), heading: (0, -1), action: forward, reward: -10.9931896667
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': -10.993189666728009, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.9931896667, Q[state]-after: {'forward': -5.4965948333640045, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.99)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.9064 < 0.9500
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: forward, reward: 0.444870892295
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': 0.4448708922952197, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.444870892295, Q[state]-after: {'forward': 0.22243544614760985, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.44)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.6488 < 0.9500
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: left, reward: 1.9399614048
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 1.9399614047988323, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.9399614048, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9699807023994161}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 1.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.4473 < 0.9500
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: None, reward: -5.46543736274
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', 'right', 'forward'), 'deadline': 18, 't': 7, 'action': None, 'reward': -5.465437362737358, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'right', 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.46543736274, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.732718681368679, 'left': 0.0}
Agent previous state: ('forward', 'green', 'right', 'right', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.7757 < 0.9500
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: None, reward: -4.78753124106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': -4.787531241060332, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -4.78753124106, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.393765620530166, 'left': 0.0}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.79)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.7200 < 0.9500
Environment.act() [POST]: location: (8, 5), heading: (0, -1), action: right, reward: 0.696214694538
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 0.6962146945375751, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.696214694538, Q[state]-after: {'forward': 0.0, 'right': 0.34810734726878756, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.70)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.4292 < 0.9500
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: forward, reward: 1.49170949644
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.4917094964416724, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.49170949644, Q[state]-after: {'forward': 0.7458547482208362, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 1.49)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.4102 < 0.9500
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: left, reward: -9.02202729271
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -9.022027292705111, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.02202729271, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.511013646352556}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.02)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.1286 < 0.9500
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: left, reward: -10.6088379384
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': -10.608837938365305, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.511013646352556}, action: left, reward: -10.6088379384, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.3165 < 0.9500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 0.625241833766
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 0.625241833766085, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.625241833766, Q[state]-after: {'forward': 0.0, 'right': 0.3126209168830425, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.63)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.0193 < 0.9500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: -10.2847016751
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': -10.284701675142642, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.2847016751, Q[state]-after: {'forward': -5.142350837571321, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.28)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.3617 < 0.9500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: -5.03203820163
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -5.032038201630409, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.03203820163, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.5160191008152046, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.03)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.4477 < 0.9500
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: 0.897181729087
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': 0.8971817290870768, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.5160191008152046, 'left': 0.0}, action: left, reward: 0.897181729087, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.5160191008152046, 'left': 0.4485908645435384}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 0.90)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.8418 < 0.9500
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: -9.61971304605
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -9.619713046045895, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.61971304605, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.8098565230229475}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.62)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.8329 < 0.9500
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: None, reward: 2.26213147771
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': None, 'reward': 2.262131477706813, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -7.55992579235893}, action: None, reward: 2.26213147771, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.1310657388534064, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.9974 >= 0.9500
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: forward, reward: -39.3282116103
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -39.32821161029295, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.3282116103, Q[state]-after: {'forward': -19.664105805146477, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.33)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.7878 < 0.9500
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: right, reward: -0.470474954979
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': -0.4704749549788745, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -0.470474954979, Q[state]-after: {'forward': 0.0, 'right': -0.23523747748943724, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent drove right instead of left. (rewarded -0.47)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.1089 < 0.9500
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: None, reward: 1.94832507177
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 1.9483250717747889, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -5.142350837571321, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 1.94832507177, Q[state]-after: {'forward': -5.142350837571321, 'right': 0.0, None: 0.9741625358873944, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.0279 < 0.9500
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: left, reward: -20.9586715277
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 3, 't': 22, 'action': 'left', 'reward': -20.958671527694953, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -20.9586715277, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.479335763847477}
Agent previous state: ('right', 'green', None, None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.96)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.7547 < 0.9500
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: forward, reward: 0.814270350033
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': 0.8142703500326804, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}, action: forward, reward: 0.814270350033, Q[state]-after: {'forward': 0.4071351750163402, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.81)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.6868 < 0.9500
Environment.act() [POST]: location: (3, 3), heading: (1, 0), action: forward, reward: -10.2007387644
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -10.200738764447589, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -5.142350837571321, 'right': 0.0, None: 0.9741625358873944, 'left': 0.0}, action: forward, reward: -10.2007387644, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.0, None: 0.9741625358873944, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.20)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 2
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (5, 6), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.0643 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: left, reward: -40.1999600895
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'left'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -40.19996008946437, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -40.1999600895, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -20.099980044732185}
Agent previous state: ('left', 'red', None, 'forward', 'left')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.2627 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 1.25121854873
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.2512185487290404, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 1.25121854873, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.6256092743645202, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.25)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.3904 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 1.04421131536
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 23, 't': 2, 'action': None, 'reward': 1.0442113153571841, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.6256092743645202, 'left': 0.0}, action: None, reward: 1.04421131536, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.8349102948608522, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.5146 < 0.9000
Environment.act() [POST]: location: (1, 6), heading: (0, -1), action: left, reward: 1.88720383443
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 1.8872038344296787, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.88720383443, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9436019172148393}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.8850 < 0.9000
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: right, reward: 1.53089130321
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 1.5308913032136773, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.1310657388534064, 'left': -7.55992579235893}, action: right, reward: 1.53089130321, Q[state]-after: {'forward': 0.0, 'right': 0.7654456516068386, None: 1.1310657388534064, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.6794 < 0.9000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: 0.724693951943
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 0.7246939519432462, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.724693951943, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.3623469759716231}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.72)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.7571 < 0.9000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: left, reward: -9.7678431952
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': -9.767843195198532, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.7678431952, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.883921597599266}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -9.77)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.0121 < 0.9000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: 2.19568675115
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': 2.1956867511475187, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.0, None: 0.9741625358873944, 'left': 0.0}, action: None, reward: 2.19568675115, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.0, None: 1.5849246435174567, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.9485 >= 0.9000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: forward, reward: -9.05341231029
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': -9.05341231029442, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.05341231029, Q[state]-after: {'forward': -4.52670615514721, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.05)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.1129 < 0.9000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: None, reward: -4.75894079063
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 16, 't': 9, 'action': None, 'reward': -4.758940790627216, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.5160191008152046, 'left': 0.4485908645435384}, action: None, reward: -4.75894079063, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.6374799457212106, 'left': 0.4485908645435384}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.1728 < 0.9000
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: left, reward: 0.959878456178
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 15, 't': 10, 'action': 'left', 'reward': 0.959878456177569, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.959878456178, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.4799392280887845}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove left instead of right. (rewarded 0.96)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.0389 < 0.9000
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 2.25357661513
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 14, 't': 11, 'action': None, 'reward': 2.2535766151328507, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.25357661513, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.1267883075664253, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.25)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.3660 < 0.9000
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: None, reward: 2.50085727005
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': 2.50085727004858, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.1267883075664253, 'left': 0.0}, action: None, reward: 2.50085727005, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.8138227888075025, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.4759 < 0.9000
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: left, reward: 1.61026749663
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 1.610267496631629, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.3623469759716231}, action: left, reward: 1.61026749663, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9863072363016261}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.61)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.7771 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: 0.904712888559
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 11, 't': 14, 'action': 'forward', 'reward': 0.9047128885588681, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.4071351750163402, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}, action: forward, reward: 0.904712888559, Q[state]-after: {'forward': 0.6559240317876042, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.90)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.7547 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: -10.1345099539
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -10.134509953878924, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.1345099539, Q[state]-after: {'forward': -5.067254976939462, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.13)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.3482 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: left, reward: -39.1834565525
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'forward'), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -39.183456552487364, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -39.1834565525, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.591728276243682}
Agent previous state: ('right', 'red', None, 'forward', 'forward')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.18)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.0838 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: left, reward: -20.1654656995
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'forward'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -20.165465699549618, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -20.1654656995, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.082732849774809}
Agent previous state: ('right', 'green', 'right', 'forward', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.17)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.5073 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: None, reward: -5.22557535579
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', None), 'deadline': 7, 't': 18, 'action': None, 'reward': -5.225575355794129, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.22557535579, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.6127876778970647, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'forward', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.23)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.3088 < 0.9000
Environment.act() [POST]: location: (1, 7), heading: (0, 1), action: forward, reward: -39.4342021796
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', None), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': -39.43420217955256, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.4342021796, Q[state]-after: {'forward': -19.71710108977628, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.43)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.5387 < 0.9000
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: right, reward: 1.04835170866
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 1.0483517086635488, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.0, None: 1.5849246435174567, 'left': 0.0}, action: right, reward: 1.04835170866, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.5849246435174567, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.05)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.8870 < 0.9000
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: forward, reward: -9.35003337175
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -9.350033371752614, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.35003337175, Q[state]-after: {'forward': -4.675016685876307, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.35)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.6008 < 0.9000
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: None, reward: 1.96568571616
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'right'), 'deadline': 3, 't': 22, 'action': None, 'reward': 1.9656857161590686, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 1.96568571616, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.9828428580795343, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, 'right')
Agent properly idled at a red light. (rewarded 1.97)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.8713 < 0.9000
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: left, reward: -9.5196657168
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -9.519665716800834, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.5196657168, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.759832858400417}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -9.52)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.1063 < 0.9000
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: right, reward: 0.604124368901
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.6041243689008093, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.8138227888075025, 'left': 0.0}, action: right, reward: 0.604124368901, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8138227888075025, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, None, None)
Agent drove right instead of forward. (rewarded 0.60)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 3
\-------------------------

Environment.reset(): Trial set up with start = (5, 2), destination = (2, 5), deadline = 30

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.4954 < 0.8500
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 0.0399513113949
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'right', 'left'), 'deadline': 30, 't': 0, 'action': 'left', 'reward': 0.03995131139485064, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.0399513113949, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.01997565569742532}
Agent previous state: ('right', 'green', 'forward', 'right', 'left')
Agent drove left instead of right. (rewarded 0.04)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.8585 >= 0.8500
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: forward, reward: -9.67309387582
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 29, 't': 1, 'action': 'forward', 'reward': -9.673093875816608, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.67309387582, Q[state]-after: {'forward': -4.836546937908304, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light. (rewarded -9.67)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.4380 < 0.8500
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: forward, reward: -10.502122932
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': -10.502122932009371, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.836546937908304, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.502122932, Q[state]-after: {'forward': -7.669334934958838, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent attempted driving forward through a red light. (rewarded -10.50)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.1495 < 0.8500
Environment.act() [POST]: location: (7, 2), heading: (1, 0), action: forward, reward: 2.21835899803
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': 2.2183589980329272, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.21835899803, Q[state]-after: {'forward': 1.1091794990164636, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 2.22)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.6596 < 0.8500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: 1.33051285574
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 26, 't': 4, 'action': 'left', 'reward': 1.3305128557440487, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.33051285574, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.6652564278720243}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.33)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.1077 < 0.8500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: 2.43221955862
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 25, 't': 5, 'action': None, 'reward': 2.4322195586213855, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.43221955862, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.2161097793106928, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 2.43)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.1401 < 0.8500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: 1.24506682898
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': None, 'reward': 1.2450668289849387, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.5849246435174567, 'left': 0.0}, action: None, reward: 1.24506682898, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.25)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.5225 < 0.8500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: left, reward: -9.02475141803
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 23, 't': 7, 'action': 'left', 'reward': -9.024751418030242, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.883921597599266}, action: left, reward: -9.02475141803, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -6.954336507814753}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving left through a red light. (rewarded -9.02)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.5086 < 0.8500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: forward, reward: -9.36447010971
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': -9.36447010971137, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.36447010971, Q[state]-after: {'forward': -4.682235054855685, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -9.36)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.4203 < 0.8500
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: 1.32230161682
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 21, 't': 9, 'action': 'left', 'reward': 1.3223016168179493, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.6374799457212106, 'left': 0.4485908645435384}, action: left, reward: 1.32230161682, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.6374799457212106, 'left': 0.8854462406807438}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.32)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.2728 < 0.8500
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: -9.29861193824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 20, 't': 10, 'action': 'left', 'reward': -9.298611938244374, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': 0.0}, action: left, reward: -9.29861193824, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -4.649305969122187}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.30)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.1981 < 0.8500
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: right, reward: -20.3367269529
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 19, 't': 11, 'action': 'right', 'reward': -20.336726952939102, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -20.3367269529, Q[state]-after: {'forward': 0.0, 'right': -10.168363476469551, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.34)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.1508 < 0.8500
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: -9.69769865203
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': -9.697698652031852, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -4.649305969122187}, action: left, reward: -9.69769865203, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -7.173502310577019}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.70)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.1276 < 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: forward, reward: 1.73987346763
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 17, 't': 13, 'action': 'forward', 'reward': 1.73987346762749, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.73987346763, Q[state]-after: {'forward': 0.869936733813745, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 1.74)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.0930 < 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: 2.22681505202
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'right'), 'deadline': 16, 't': 14, 'action': None, 'reward': 2.22681505202336, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.22681505202, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.11340752601168, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'right', 'right')
Agent properly idled at a red light. (rewarded 2.23)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.4060 < 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: -5.55462141001
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 15, 't': 15, 'action': None, 'reward': -5.554621410013094, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9863072363016261}, action: None, reward: -5.55462141001, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.777310705006547, 'left': 0.9863072363016261}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.6202 < 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: -4.69414831244
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 14, 't': 16, 'action': None, 'reward': -4.694148312437841, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.777310705006547, 'left': 0.9863072363016261}, action: None, reward: -4.69414831244, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.9863072363016261}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.69)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.9540 >= 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: None, reward: 2.03384262906
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 13, 't': 17, 'action': None, 'reward': 2.0338426290572924, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8138227888075025, 'left': 0.0}, action: None, reward: 2.03384262906, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.9238327089323974, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.6393 < 0.8500
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: left, reward: -9.29924172158
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 12, 't': 18, 'action': 'left', 'reward': -9.299241721575774, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.9238327089323974, 'left': 0.0}, action: left, reward: -9.29924172158, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.9238327089323974, 'left': -4.649620860787887}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.30)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.8527 >= 0.8500
Environment.act() [POST]: location: (5, 2), heading: (0, 1), action: left, reward: 0.57577030072
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': 0.5757703007195735, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.9863072363016261}, action: left, reward: 0.57577030072, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.7810387685105997}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.58)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.5143 < 0.8500
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: 0.0189154777961
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'left'), 'deadline': 10, 't': 20, 'action': 'left', 'reward': 0.018915477796138447, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.4799392280887845}, action: left, reward: 0.0189154777961, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.24942735294246146}
Agent previous state: ('right', 'green', 'left', None, 'left')
Agent drove left instead of right. (rewarded 0.02)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.3622 < 0.8500
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: left, reward: -10.0470730144
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 9, 't': 21, 'action': 'left', 'reward': -10.047073014443942, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.23573483606494128, None: 0.0, 'left': -5.001187520011512}, action: left, reward: -10.0470730144, Q[state]-after: {'forward': 0.0, 'right': 0.23573483606494128, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent attempted driving left through a red light. (rewarded -10.05)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.8380 < 0.8500
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 0.29552842886
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', 'left', 'left'), 'deadline': 8, 't': 22, 'action': 'right', 'reward': 0.29552842885988206, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.29552842886, Q[state]-after: {'forward': 0.0, 'right': 0.14776421442994103, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', 'left', 'left')
Agent drove right instead of forward. (rewarded 0.30)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.7789 < 0.8500
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: -9.36846187431
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': -9.368461874305613, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.8349102948608522, 'left': 0.0}, action: forward, reward: -9.36846187431, Q[state]-after: {'forward': -4.684230937152806, 'right': 0.0, None: 0.8349102948608522, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -9.37)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.5557 < 0.8500
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: right, reward: 0.659460500326
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 0.6594605003261105, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -4.684230937152806, 'right': 0.0, None: 0.8349102948608522, 'left': 0.0}, action: right, reward: 0.659460500326, Q[state]-after: {'forward': -4.684230937152806, 'right': 0.32973025016305524, None: 0.8349102948608522, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 0.66)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
Agent selected maxQ 0.8522 >= 0.8500
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: left, reward: -0.0795850619761
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': -0.0795850619761409, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.7810387685105997}, action: left, reward: -0.0795850619761, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.3507268532672294}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded -0.08)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
Agent decided to explore 0.7446 < 0.8500
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: left, reward: -9.85900373462
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 4, 't': 26, 'action': 'left', 'reward': -9.859003734620226, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -7.173502310577019}, action: left, reward: -9.85900373462, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -8.516253022598622}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.86)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
Agent decided to explore 0.5882 < 0.8500
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 2.08811817687
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 3, 't': 27, 'action': 'right', 'reward': 2.088118176866749, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.08811817687, Q[state]-after: {'forward': 0.0, 'right': 1.0440590884333745, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.09)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
Agent decided to explore 0.2959 < 0.8500
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: None, reward: 2.10799781188
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 2, 't': 28, 'action': None, 'reward': 2.107997811878578, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.10799781188, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.053998905939289, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 2.11)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
Agent decided to explore 0.4233 < 0.8500
Environment.act() [POST]: location: (4, 3), heading: (0, -1), action: right, reward: -0.884487534055
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'forward'), 'deadline': 1, 't': 29, 'action': 'right', 'reward': -0.8844875340550031, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -0.884487534055, Q[state]-after: {'forward': 0.0, 'right': -0.44224376702750157, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'left', None, 'forward')
Agent drove right instead of forward. (rewarded -0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 4
\-------------------------

Environment.reset(): Trial set up with start = (2, 2), destination = (6, 7), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.3515 < 0.8000
Environment.act() [POST]: location: (1, 2), heading: (-1, 0), action: right, reward: 2.60017946038
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 2.600179460378786, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.60017946038, Q[state]-after: {'forward': 0.0, 'right': 1.300089730189393, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.5014 < 0.8000
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: left, reward: 0.314312895811
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', 'left'), 'deadline': 24, 't': 1, 'action': 'left', 'reward': 0.31431289581085475, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.314312895811, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.15715644790542738}
Agent previous state: ('forward', 'green', 'left', 'left', 'left')
Agent drove left instead of forward. (rewarded 0.31)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.0690 < 0.8000
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: right, reward: 2.56538899943
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', 'right'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 2.5653889994265278, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'forward', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.56538899943, Q[state]-after: {'forward': 0.0, 'right': 1.2826944997132639, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'forward', 'right')
Agent followed the waypoint right. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.6094 < 0.8000
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: -39.7717045446
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 22, 't': 3, 'action': 'left', 'reward': -39.77170454462899, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -39.7717045446, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.885852272314494}
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.77)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.0964 < 0.8000
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: None, reward: 1.84464007369
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.8446400736880313, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.9238327089323974, 'left': -4.649620860787887}, action: None, reward: 1.84464007369, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8842363913102145, 'left': -4.649620860787887}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9254 >= 0.8000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: 1.07636171347
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 1.0763617134734305, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.3507268532672294}, action: left, reward: 1.07636171347, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.7135442833703299}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.08)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.9299 >= 0.8000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: forward, reward: 0.349093089398
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 0.34909308939843353, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.349093089398, Q[state]-after: {'forward': 0.17454654469921677, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent drove forward instead of right. (rewarded 0.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.0915 < 0.8000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: forward, reward: -40.914373166
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -40.914373166024575, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': -10.168363476469551, None: 0.0, 'left': 0.0}, action: forward, reward: -40.914373166, Q[state]-after: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.91)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.1237 < 0.8000
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 1.08825293703
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'right'), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.0882529370311116, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.08825293703, Q[state]-after: {'forward': 0.0, 'right': 0.5441264685155558, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', 'right')
Agent followed the waypoint right. (rewarded 1.09)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.7224 < 0.8000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: right, reward: 1.20512381679
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 1.2051238167898033, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.20512381679, Q[state]-after: {'forward': 0.0, 'right': 0.6025619083949016, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent drove right instead of forward. (rewarded 1.21)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.7400 < 0.8000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: None, reward: 2.53178852961
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 15, 't': 10, 'action': None, 'reward': 2.531788529610931, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.53178852961, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.2658942648054654, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.7014 < 0.8000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: forward, reward: -9.35650679359
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': -9.356506793585678, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', None), Q[state]-before: {'forward': -5.4965948333640045, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.35650679359, Q[state]-after: {'forward': -7.426550813474842, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.36)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.7983 < 0.8000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: left, reward: -9.03311023094
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': -9.033110230941022, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.03311023094, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.516555115470511}
Agent previous state: ('left', 'red', 'right', None, None)
Agent attempted driving left through a red light. (rewarded -9.03)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.8461 >= 0.8000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: forward, reward: 0.177448629129
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'forward', 'reward': 0.17744862912913106, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.22243544614760985, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.177448629129, Q[state]-after: {'forward': 0.19994203763837046, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.6571 < 0.8000
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: right, reward: 1.46067919144
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 1.4606791914405108, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.7654456516068386, None: 1.1310657388534064, 'left': -7.55992579235893}, action: right, reward: 1.46067919144, Q[state]-after: {'forward': 0.0, 'right': 1.1130624215236748, None: 1.1310657388534064, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 1.46)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.5895 < 0.8000
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: None, reward: 1.84104455847
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.8410445584676602, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 1.1130624215236748, None: 1.1310657388534064, 'left': -7.55992579235893}, action: None, reward: 1.84104455847, Q[state]-after: {'forward': 0.0, 'right': 1.1130624215236748, None: 1.4860551486605333, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.3807 < 0.8000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: right, reward: 0.885578629522
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 0.8855786295220284, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 1.1130624215236748, None: 1.4860551486605333, 'left': -7.55992579235893}, action: right, reward: 0.885578629522, Q[state]-after: {'forward': 0.0, 'right': 0.9993205255228517, None: 1.4860551486605333, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.89)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.5150 < 0.8000
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: left, reward: 1.53514065543
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 8, 't': 17, 'action': 'left', 'reward': 1.5351406554320712, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.53514065543, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.7675703277160356}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 1.54)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.0181 < 0.8000
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: forward, reward: -39.0932120679
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -39.09321206790797, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.2658942648054654, 'left': 0.0}, action: forward, reward: -39.0932120679, Q[state]-after: {'forward': -19.546606033953985, 'right': 0.0, None: 1.2658942648054654, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.09)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.1635 < 0.8000
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: -5.53874396237
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'forward', 'forward'), 'deadline': 6, 't': 19, 'action': None, 'reward': -5.538743962370097, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.53874396237, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.7693719811850483, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.1519 < 0.8000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: forward, reward: 1.07280277969
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', None), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': 1.0728027796916797, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.07280277969, Q[state]-after: {'forward': 0.5364013898458398, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', 'forward', None)
Agent drove forward instead of left. (rewarded 1.07)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.7132 < 0.8000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: 2.08800009163
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'right', 'right', None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.088000091632421, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.08800009163, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.0440000458162104, 'left': 0.0}
Agent previous state: ('left', 'red', 'right', 'right', None)
Agent properly idled at a red light. (rewarded 2.09)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.5580 < 0.8000
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: right, reward: 0.975183689476
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': 0.9751836894762926, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.9993205255228517, None: 1.4860551486605333, 'left': -7.55992579235893}, action: right, reward: 0.975183689476, Q[state]-after: {'forward': 0.0, 'right': 0.9872521074995722, None: 1.4860551486605333, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.98)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.4380 < 0.8000
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: forward, reward: -0.473568806358
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': -0.4735688063581005, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.6559240317876042, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}, action: forward, reward: -0.473568806358, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded -0.47)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.8003 >= 0.8000
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: 0.030972640275
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': 0.030972640275017227, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.030972640275, Q[state]-after: {'forward': 0.015486320137508613, 'right': 0.0, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove forward instead of right. (rewarded 0.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 5
\-------------------------

Environment.reset(): Trial set up with start = (5, 3), destination = (2, 7), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.8667 >= 0.7500
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: 0.820284204828
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 0.8202842048277181, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.820284204828, Q[state]-after: {'forward': 0.41014210241385907, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove forward instead of right. (rewarded 0.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.0735 < 0.7500
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: right, reward: 2.81685101233
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 2.8168510123323847, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.479335763847477}, action: right, reward: 2.81685101233, Q[state]-after: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -10.479335763847477}
Agent previous state: ('right', 'green', None, None, 'right')
Agent followed the waypoint right. (rewarded 2.82)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.3861 < 0.7500
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: None, reward: -5.5298801766
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 23, 't': 2, 'action': None, 'reward': -5.529880176595835, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.7357295087221942, 'left': 0.7135442833703299}, action: None, reward: -5.5298801766, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.632804842659015, 'left': 0.7135442833703299}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9370 >= 0.7500
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: 0.343748453816
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 22, 't': 3, 'action': 'left', 'reward': 0.3437484538158746, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.632804842659015, 'left': 0.7135442833703299}, action: left, reward: 0.343748453816, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.632804842659015, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.34)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.1522 < 0.7500
Environment.act() [POST]: location: (3, 5), heading: (-1, 0), action: right, reward: 1.64214064568
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 1.6421406456793242, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.64214064568, Q[state]-after: {'forward': 0.0, 'right': 0.8210703228396621, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent followed the waypoint right. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.6457 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 1.75633435782
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.7563343578186164, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.632804842659015, 'left': 0.5286463685931022}, action: right, reward: 1.75633435782, Q[state]-after: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.632804842659015, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.76)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.0341 < 0.7500
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: forward, reward: 0.520520369457
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 0.520520369457114, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.3126209168830425, None: 0.0, 'left': 0.0}, action: forward, reward: 0.520520369457, Q[state]-after: {'forward': 0.260260184728557, 'right': 0.3126209168830425, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove forward instead of left. (rewarded 0.52)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.5836 < 0.7500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: right, reward: 1.60715329235
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.6071532923514937, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.60715329235, Q[state]-after: {'forward': 0.0, 'right': 0.8035766461757469, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.2141 < 0.7500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 2.38161167567
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.381611675666724, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.8098565230229475}, action: None, reward: 2.38161167567, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.190805837833362, 'left': -4.8098565230229475}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.38)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.1832 < 0.7500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: -4.36913518088
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 16, 't': 9, 'action': None, 'reward': -4.369135180876717, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', None), Q[state]-before: {'forward': 0.7458547482208362, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -4.36913518088, Q[state]-after: {'forward': 0.7458547482208362, 'right': 0.0, None: -2.1845675904383586, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.37)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.6010 < 0.7500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: -5.66634041554
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', 'right', 'left', None), 'deadline': 15, 't': 10, 'action': None, 'reward': -5.666340415544871, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.66634041554, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.8331702077724357, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', 'left', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.67)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.6683 < 0.7500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: right, reward: 1.44034931995
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'left'), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 1.4403493199508763, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.44034931995, Q[state]-after: {'forward': 0.0, 'right': 0.7201746599754382, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', 'left')
Agent drove right instead of left. (rewarded 1.44)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.6279 < 0.7500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: 1.85069943949
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': None, 'reward': 1.8506994394853695, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.4149957362511976, 'left': -8.516253022598622}, action: None, reward: 1.85069943949, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6328475878682835, 'left': -8.516253022598622}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.7993 >= 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 1.58138461362
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.5813846136188772, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.3878195942593607, None: 0.0, 'left': 0.0}, action: right, reward: 1.58138461362, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.4846021039391188, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.58)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.5658 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: 2.23705833999
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 11, 't': 14, 'action': None, 'reward': 2.237058339990673, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 2.23705833999, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.1185291699953366, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 2.24)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.0358 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: -4.16628719601
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': -4.166287196011418, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.632804842659015, 'left': 0.5286463685931022}, action: None, reward: -4.16628719601, Q[state]-after: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.399546019335217, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.17)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.6361 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: -5.35663669818
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 16, 'action': None, 'reward': -5.356636698181602, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.399546019335217, 'left': 0.5286463685931022}, action: None, reward: -5.35663669818, Q[state]-after: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.878091358758409, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.36)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.2589 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: 1.41140184104
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 8, 't': 17, 'action': None, 'reward': 1.4114018410357796, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.6025619083949016, None: 0.0, 'left': 0.0}, action: None, reward: 1.41140184104, Q[state]-after: {'forward': 0.0, 'right': 0.6025619083949016, None: 0.7057009205178898, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.41)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.5485 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: -10.5126473691
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': 'left', 'reward': -10.512647369148015, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8842363913102145, 'left': -4.649620860787887}, action: left, reward: -10.5126473691, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8842363913102145, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.51)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.4368 < 0.7500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: -0.137552606142
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 6, 't': 19, 'action': None, 'reward': -0.13755260614190756, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -0.137552606142, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded -0.14)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.4113 < 0.7500
Environment.act() [POST]: location: (3, 3), heading: (0, -1), action: right, reward: 1.10369530438
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 1.103695304383376, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.393765620530166, 'left': 0.0}, action: right, reward: 1.10369530438, Q[state]-after: {'forward': 0.0, 'right': 0.551847652191688, None: -2.393765620530166, 'left': 0.0}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent drove right instead of forward. (rewarded 1.10)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.1456 < 0.7500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: right, reward: 0.397360285039
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 4, 't': 21, 'action': 'right', 'reward': 0.3973602850394393, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.19994203763837046, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.397360285039, Q[state]-after: {'forward': 0.19994203763837046, 'right': 0.19868014251971966, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.40)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.4811 < 0.7500
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: forward, reward: 0.236665876688
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 3, 't': 22, 'action': 'forward', 'reward': 0.23666587668767125, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9699807023994161}, action: forward, reward: 0.236665876688, Q[state]-after: {'forward': 0.11833293834383563, 'right': 0.0, None: 0.0, 'left': 0.9699807023994161}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove forward instead of left. (rewarded 0.24)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.5591 < 0.7500
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: right, reward: 0.971052358454
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', None), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.9710523584544188, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.971052358454, Q[state]-after: {'forward': 0.0, 'right': 0.4855261792272094, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'forward', None)
Agent drove right instead of left. (rewarded 0.97)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.8916 >= 0.7500
Environment.act() [POST]: location: (5, 4), heading: (0, 1), action: forward, reward: -10.376870569
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -10.376870568968506, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.376870569, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.0, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.38)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 6
\-------------------------

Environment.reset(): Trial set up with start = (6, 5), destination = (2, 2), deadline = 35

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.9533 >= 0.7000
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: 1.53890437927
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 35, 't': 0, 'action': 'right', 'reward': 1.5389043792663648, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.8781671789093082, None: -4.878091358758409, 'left': 0.5286463685931022}, action: right, reward: 1.53890437927, Q[state]-after: {'forward': 0.0, 'right': 1.2085357790878364, None: -4.878091358758409, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded 1.54)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.6457 < 0.7000
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: forward, reward: -9.31758316837
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 34, 't': 1, 'action': 'forward', 'reward': -9.317583168371522, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.31758316837, Q[state]-after: {'forward': -4.658791584185761, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent attempted driving forward through a red light. (rewarded -9.32)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.7055 >= 0.7000
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: None, reward: 2.10811082795
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 33, 't': 2, 'action': None, 'reward': 2.1081108279498597, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -4.684230937152806, 'right': 0.32973025016305524, None: 0.8349102948608522, 'left': 0.0}, action: None, reward: 2.10811082795, Q[state]-after: {'forward': -4.684230937152806, 'right': 0.32973025016305524, None: 1.4715105614053559, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.11)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.5154 < 0.7000
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: left, reward: -9.2923134645
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 32, 't': 3, 'action': 'left', 'reward': -9.292313464499616, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': -0.23523747748943724, None: 0.0, 'left': 0.0}, action: left, reward: -9.2923134645, Q[state]-after: {'forward': 0.0, 'right': -0.23523747748943724, None: 0.0, 'left': -4.646156732249808}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent attempted driving left through a red light. (rewarded -9.29)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.8749 >= 0.7000
Environment.act() [POST]: location: (6, 6), heading: (0, 1), action: right, reward: -20.2383724729
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'left'), 'deadline': 31, 't': 4, 'action': 'right', 'reward': -20.238372472939176, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, 'left'), Q[state]-before: {'forward': -19.664105805146477, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -20.2383724729, Q[state]-after: {'forward': -19.664105805146477, 'right': -10.119186236469588, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, 'left')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.24)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.1359 < 0.7000
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: right, reward: 0.449101185533
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 30, 't': 5, 'action': 'right', 'reward': 0.4491011855330096, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -4.684230937152806, 'right': 0.32973025016305524, None: 1.4715105614053559, 'left': 0.0}, action: right, reward: 0.449101185533, Q[state]-after: {'forward': -4.684230937152806, 'right': 0.3894157178480324, None: 1.4715105614053559, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 0.45)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.3687 < 0.7000
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: right, reward: 0.92226500783
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 29, 't': 6, 'action': 'right', 'reward': 0.9222650078304091, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.92226500783, Q[state]-after: {'forward': 0.0, 'right': 0.46113250391520455, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove right instead of forward. (rewarded 0.92)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.3284 < 0.7000
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: -4.56399105033
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 28, 't': 7, 'action': None, 'reward': -4.5639910503345, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.19994203763837046, 'right': 0.19868014251971966, None: 0.0, 'left': 0.0}, action: None, reward: -4.56399105033, Q[state]-after: {'forward': 0.19994203763837046, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.56)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.7659 >= 0.7000
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: forward, reward: 1.04538807357
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'forward'), 'deadline': 27, 't': 8, 'action': 'forward', 'reward': 1.0453880735673238, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.04538807357, Q[state]-after: {'forward': 0.5226940367836619, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, 'forward')
Agent drove forward instead of left. (rewarded 1.05)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.7737 >= 0.7000
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: 1.77865865163
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 26, 't': 9, 'action': 'forward', 'reward': 1.7786586516273968, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.19994203763837046, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}, action: forward, reward: 1.77865865163, Q[state]-after: {'forward': 0.9893003446328836, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.78)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.5727 < 0.7000
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: left, reward: -40.2926134574
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 25, 't': 10, 'action': 'left', 'reward': -40.29261345736761, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -40.2926134574, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.29)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.8570 >= 0.7000
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: -40.5424209104
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 24, 't': 11, 'action': 'forward', 'reward': -40.54242091039493, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}, action: forward, reward: -40.5424209104, Q[state]-after: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.54)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.2752 < 0.7000
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: left, reward: -9.43897838957
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 23, 't': 12, 'action': 'left', 'reward': -9.43897838956752, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.190805837833362, 'left': -4.8098565230229475}, action: left, reward: -9.43897838957, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.190805837833362, 'left': -7.124417456295234}
Agent previous state: ('left', 'red', 'left', None, None)
Agent attempted driving left through a red light. (rewarded -9.44)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.8521 >= 0.7000
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: 1.13790002927
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 22, 't': 13, 'action': 'forward', 'reward': 1.1379000292666732, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.9893003446328836, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}, action: forward, reward: 1.13790002927, Q[state]-after: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.3361 < 0.7000
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: right, reward: 0.111288257628
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'forward', 'left'), 'deadline': 21, 't': 14, 'action': 'right', 'reward': 0.11128825762778038, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.111288257628, Q[state]-after: {'forward': 0.0, 'right': 0.05564412881389019, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', 'forward', 'left')
Agent drove right instead of left. (rewarded 0.11)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.0273 < 0.7000
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: None, reward: -4.96258730533
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 15, 'action': None, 'reward': -4.962587305326441, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 1.2085357790878364, None: -4.878091358758409, 'left': 0.5286463685931022}, action: None, reward: -4.96258730533, Q[state]-after: {'forward': 0.0, 'right': 1.2085357790878364, None: -4.920339332042425, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.96)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.2117 < 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 1.65310300715
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 19, 't': 16, 'action': 'right', 'reward': 1.6531030071513797, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.65310300715, Q[state]-after: {'forward': 0.0, 'right': 0.8265515035756898, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 1.65)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.1508 < 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: -9.12414908771
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 18, 't': 17, 'action': 'forward', 'reward': -9.124149087710787, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.12414908771, Q[state]-after: {'forward': -4.562074543855394, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.12)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.9367 >= 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: None, reward: 1.32417985278
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 18, 'action': None, 'reward': 1.3241798527784718, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.9872521074995722, None: 1.4860551486605333, 'left': -7.55992579235893}, action: None, reward: 1.32417985278, Q[state]-after: {'forward': 0.0, 'right': 0.9872521074995722, None: 1.4051175007195025, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.5744 < 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: -9.16325812432
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 19, 'action': 'forward', 'reward': -9.163258124315066, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.9872521074995722, None: 1.4051175007195025, 'left': -7.55992579235893}, action: forward, reward: -9.16325812432, Q[state]-after: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 1.4051175007195025, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.16)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.6910 < 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: left, reward: -9.34785216403
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 15, 't': 20, 'action': 'left', 'reward': -9.347852164031304, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.34785216403, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.673926082015652}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light. (rewarded -9.35)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.2410 < 0.7000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: left, reward: -20.8178883282
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 14, 't': 21, 'action': 'left', 'reward': -20.81788832816735, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.8035766461757469, None: 0.0, 'left': 0.0}, action: left, reward: -20.8178883282, Q[state]-after: {'forward': 0.0, 'right': 0.8035766461757469, None: 0.0, 'left': -10.408944164083675}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.82)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.4089 < 0.7000
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: left, reward: 0.761667974576
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 13, 't': 22, 'action': 'left', 'reward': 0.761667974575958, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.761667974576, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.380833987287979}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent followed the waypoint left. (rewarded 0.76)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.1167 < 0.7000
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: None, reward: -4.97559889202
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 12, 't': 23, 'action': None, 'reward': -4.975598892019448, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -4.97559889202, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.98)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.7021 >= 0.7000
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: 2.56059867718
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 11, 't': 24, 'action': 'forward', 'reward': 2.5605986771816935, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}, action: forward, reward: 2.56059867718, Q[state]-after: {'forward': 1.2802993385908468, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 2.56)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
Agent decided to explore 0.4465 < 0.7000
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: None, reward: -4.8492609838
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', 'right', None, None), 'deadline': 10, 't': 25, 'action': None, 'reward': -4.849260983799049, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.551847652191688, None: -2.393765620530166, 'left': 0.0}, action: None, reward: -4.8492609838, Q[state]-after: {'forward': 0.0, 'right': 0.551847652191688, None: -3.6215133021646073, 'left': 0.0}
Agent previous state: ('forward', 'green', 'right', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.85)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
Agent decided to explore 0.4086 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: right, reward: -0.356010107829
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 9, 't': 26, 'action': 'right', 'reward': -0.3560101078293776, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 1.2085357790878364, None: -4.920339332042425, 'left': 0.5286463685931022}, action: right, reward: -0.356010107829, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.920339332042425, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove right instead of forward. (rewarded -0.36)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
Agent selected maxQ 0.9847 >= 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: forward, reward: -40.631184107
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', None), 'deadline': 8, 't': 27, 'action': 'forward', 'reward': -40.63118410703403, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.631184107, Q[state]-after: {'forward': -20.315592053517015, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'left', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.63)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
Agent decided to explore 0.3022 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: -9.07630344324
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 7, 't': 28, 'action': 'left', 'reward': -9.076303443235968, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.673926082015652}, action: left, reward: -9.07630344324, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -6.87511476262581}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light. (rewarded -9.08)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
Agent decided to explore 0.5819 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: None, reward: 1.03401323756
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 6, 't': 29, 'action': None, 'reward': 1.034013237556918, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9436019172148393}, action: None, reward: 1.03401323756, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 0.9436019172148393}
Agent previous state: ('left', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.03)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Environment.step(): t = 30
Agent decided to explore 0.1945 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: None, reward: -5.6130428653
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 5, 't': 30, 'action': None, 'reward': -5.613042865301239, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.6130428653, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.8065214326506194, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.61)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Environment.step(): t = 31
Agent selected maxQ 0.7453 >= 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: None, reward: 0.333155959482
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 31, 'action': None, 'reward': 0.33315595948202836, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 1.4051175007195025, 'left': -7.55992579235893}, action: None, reward: 0.333155959482, Q[state]-after: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 0.8691367301007654, 'left': -7.55992579235893}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.33)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Environment.step(): t = 32
Agent decided to explore 0.6952 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: -9.18695415808
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 3, 't': 32, 'action': 'left', 'reward': -9.186954158082415, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 0.8691367301007654, 'left': -7.55992579235893}, action: left, reward: -9.18695415808, Q[state]-after: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 0.8691367301007654, 'left': -8.373439975220673}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.19)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Environment.step(): t = 33
Agent decided to explore 0.5093 < 0.7000
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: None, reward: -5.39743224649
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 2, 't': 33, 'action': None, 'reward': -5.3974322464864075, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -2.28199552516725, 'left': 0.0}, action: None, reward: -5.39743224649, Q[state]-after: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.0}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Environment.step(): t = 34
Agent decided to explore 0.0159 < 0.7000
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: left, reward: 1.7749275811
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 34, 'action': 'left', 'reward': 1.7749275810978535, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.0}, action: left, reward: 1.7749275811, Q[state]-after: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.8874637905489268}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.77)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 7
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (4, 4), deadline = 30

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.3305 < 0.6500
Environment.act() [POST]: location: (8, 7), heading: (-1, 0), action: forward, reward: 1.44094751068
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': 1.4409475106777756, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 1.0636001869497784, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.8874637905489268}, action: forward, reward: 1.44094751068, Q[state]-after: {'forward': 1.252273848813777, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.8874637905489268}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 1.44)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.0398 < 0.6500
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: right, reward: 0.63795910414
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 0.6379591041402229, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 1.252273848813777, 'right': 0.19868014251971966, None: -3.8397138858268285, 'left': 0.8874637905489268}, action: right, reward: 0.63795910414, Q[state]-after: {'forward': 1.252273848813777, 'right': 0.4183196233299713, None: -3.8397138858268285, 'left': 0.8874637905489268}
Agent previous state: ('left', 'green', None, None, None)
Agent drove right instead of left. (rewarded 0.64)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.3246 < 0.6500
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: -39.99189387
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': -39.99189387001206, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.99189387, Q[state]-after: {'forward': -19.99594693500603, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.99)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.4604 < 0.6500
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: -9.79054733185
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': -9.790547331852876, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.79054733185, Q[state]-after: {'forward': -4.895273665926438, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent attempted driving forward through a red light. (rewarded -9.79)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.5242 < 0.6500
Environment.act() [POST]: location: (1, 6), heading: (1, 0), action: right, reward: 2.40977443717
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 26, 't': 4, 'action': 'right', 'reward': 2.4097744371668686, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.40977443717, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.2048872185834343, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.41)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9267 >= 0.6500
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: forward, reward: 2.59611912156
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'right', None), 'deadline': 25, 't': 5, 'action': 'forward', 'reward': 2.5961191215609203, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.59611912156, Q[state]-after: {'forward': 1.2980595607804601, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', 'right', None)
Agent followed the waypoint forward. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.6585 >= 0.6500
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: right, reward: 0.794327073466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 24, 't': 6, 'action': 'right', 'reward': 0.7943270734659506, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.34810734726878756, None: 0.0, 'left': 0.0}, action: right, reward: 0.794327073466, Q[state]-after: {'forward': 0.0, 'right': 0.571217210367369, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.79)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.7851 >= 0.6500
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -40.8001334466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 23, 't': 7, 'action': 'forward', 'reward': -40.80013344663028, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.8001334466, Q[state]-after: {'forward': -20.40006672331514, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.80)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.3570 < 0.6500
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: forward, reward: -10.828831959
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': -10.82883195903175, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -4.684230937152806, 'right': 0.3894157178480324, None: 1.4715105614053559, 'left': 0.0}, action: forward, reward: -10.828831959, Q[state]-after: {'forward': -7.756531448092278, 'right': 0.3894157178480324, None: 1.4715105614053559, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.83)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.8999 >= 0.6500
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: forward, reward: 0.548603735128
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 21, 't': 9, 'action': 'forward', 'reward': 0.548603735128286, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.548603735128, Q[state]-after: {'forward': 0.274301867564143, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent drove forward instead of left. (rewarded 0.55)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.7215 >= 0.6500
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: None, reward: 1.2468401478
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 20, 't': 10, 'action': None, 'reward': 1.2468401477960986, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.190805837833362, 'left': -7.124417456295234}, action: None, reward: 1.2468401478, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.2188229928147303, 'left': -7.124417456295234}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 1.25)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.4660 < 0.6500
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: None, reward: 0.666419881653
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'left'), 'deadline': 19, 't': 11, 'action': None, 'reward': 0.6664198816533023, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 0.666419881653, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.3332099408266512, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'right', 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.67)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.8146 >= 0.6500
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: forward, reward: 1.8003273097
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'left'), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': 1.8003273097045764, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, 'left'), Q[state]-before: {'forward': 0.274301867564143, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.8003273097, Q[state]-after: {'forward': 1.0373145886343598, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, 'left')
Agent drove forward instead of left. (rewarded 1.80)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.9082 >= 0.6500
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: forward, reward: 0.672037326019
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 17, 't': 13, 'action': 'forward', 'reward': 0.672037326019282, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 1.252273848813777, 'right': 0.4183196233299713, None: -3.8397138858268285, 'left': 0.8874637905489268}, action: forward, reward: 0.672037326019, Q[state]-after: {'forward': 0.9621555874165295, 'right': 0.4183196233299713, None: -3.8397138858268285, 'left': 0.8874637905489268}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded 0.67)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.1078 < 0.6500
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: left, reward: -20.6836412916
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 16, 't': 14, 'action': 'left', 'reward': -20.683641291642356, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.8065214326506194, 'left': 0.0}, action: left, reward: -20.6836412916, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.8065214326506194, 'left': -10.341820645821178}
Agent previous state: ('left', 'green', None, None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.68)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.5191 < 0.6500
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: None, reward: -4.32413636834
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 15, 't': 15, 'action': None, 'reward': -4.324136368336578, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.9621555874165295, 'right': 0.4183196233299713, None: -3.8397138858268285, 'left': 0.8874637905489268}, action: None, reward: -4.32413636834, Q[state]-after: {'forward': 0.9621555874165295, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8874637905489268}
Agent previous state: ('left', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.0841 < 0.6500
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: right, reward: 1.49722985238
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': 1.4972298523818017, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.49722985238, Q[state]-after: {'forward': 0.0, 'right': 0.7486149261909009, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove right instead of left. (rewarded 1.50)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.1418 < 0.6500
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: None, reward: -5.05407508941
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 13, 't': 17, 'action': None, 'reward': -5.054075089414793, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.300089730189393, None: 0.0, 'left': 0.0}, action: None, reward: -5.05407508941, Q[state]-after: {'forward': 0.0, 'right': 1.300089730189393, None: -2.5270375447073965, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.05)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.6224 < 0.6500
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: forward, reward: 0.829758340578
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': 0.8297583405779101, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.829758340578, Q[state]-after: {'forward': 0.41487917028895505, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent drove forward instead of right. (rewarded 0.83)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.1603 < 0.6500
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: forward, reward: -9.97207825158
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 11, 't': 19, 'action': 'forward', 'reward': -9.972078251580793, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'left', None), Q[state]-before: {'forward': -5.067254976939462, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.97207825158, Q[state]-after: {'forward': -7.519666614260128, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.97)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.6481 < 0.6500
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: left, reward: -10.8847292738
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 10, 't': 20, 'action': 'left', 'reward': -10.88472927381668, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, None), Q[state]-before: {'forward': -4.682235054855685, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -10.8847292738, Q[state]-after: {'forward': -4.682235054855685, 'right': 0.0, None: 0.0, 'left': -5.44236463690834}
Agent previous state: ('right', 'red', 'right', None, None)
Agent attempted driving left through a red light. (rewarded -10.88)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.4382 < 0.6500
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: right, reward: 2.22379054434
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 9, 't': 21, 'action': 'right', 'reward': 2.223790544337541, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.4846021039391188, None: 0.0, 'left': 0.0}, action: right, reward: 2.22379054434, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.85419632413833, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.22)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent selected maxQ 0.7527 >= 0.6500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: forward, reward: -0.153349133238
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'right', None), 'deadline': 8, 't': 22, 'action': 'forward', 'reward': -0.15334913323793042, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.153349133238, Q[state]-after: {'forward': -0.07667456661896521, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'right', None)
Agent drove forward instead of right. (rewarded -0.15)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.9107 >= 0.6500
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: 1.23299097351
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'right'), 'deadline': 7, 't': 23, 'action': 'forward', 'reward': 1.232990973509986, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'forward', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.23299097351, Q[state]-after: {'forward': 0.616495486754993, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'forward', 'right')
Agent drove forward instead of right. (rewarded 1.23)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.0726 < 0.6500
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: -20.2908786772
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 6, 't': 24, 'action': 'right', 'reward': -20.290878677179247, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -20.2908786772, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.29)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
Agent selected maxQ 0.8587 >= 0.6500
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: -9.77405279465
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 5, 't': 25, 'action': 'forward', 'reward': -9.77405279464707, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.77405279465, Q[state]-after: {'forward': -4.887026397323535, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light. (rewarded -9.77)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
Agent decided to explore 0.3683 < 0.6500
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: right, reward: 1.52059311305
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'left'), 'deadline': 4, 't': 26, 'action': 'right', 'reward': 1.5205931130549417, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.52059311305, Q[state]-after: {'forward': 0.0, 'right': 0.7602965565274709, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', 'left')
Agent followed the waypoint right. (rewarded 1.52)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
Agent selected maxQ 0.9432 >= 0.6500
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: -39.6139476098
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', None), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -39.61394760977237, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.6139476098, Q[state]-after: {'forward': -19.806973804886184, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.61)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
Agent decided to explore 0.5797 < 0.6500
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 0.421107390004
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 2, 't': 28, 'action': None, 'reward': 0.42110739000411246, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 0.421107390004, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.21055369500205623, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent properly idled at a red light. (rewarded 0.42)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
Agent decided to explore 0.1092 < 0.6500
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: 1.38706352091
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 1, 't': 29, 'action': 'forward', 'reward': 1.3870635209129105, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.38706352091, Q[state]-after: {'forward': 0.6935317604564553, 'right': 0.0, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent followed the waypoint forward. (rewarded 1.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Environment.reset(): Trial set up with start = (4, 4), destination = (7, 3), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.4065 < 0.6000
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: left, reward: 1.68524335749
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 20, 't': 0, 'action': 'left', 'reward': 1.6852433574915011, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.015486320137508613, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.68524335749, Q[state]-after: {'forward': 0.015486320137508613, 'right': 0.0, None: 0.0, 'left': 0.8426216787457506}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove left instead of right. (rewarded 1.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.9812 >= 0.6000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: 1.54855283508
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 1.5485528350760485, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.54855283508, Q[state]-after: {'forward': 0.7742764175380242, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', 'forward', None)
Agent followed the waypoint forward. (rewarded 1.55)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6910 >= 0.6000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: right, reward: 0.483529261731
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 0.48352926173140476, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.23573483606494128, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 0.483529261731, Q[state]-after: {'forward': 0.0, 'right': 0.359632048898173, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.8566 >= 0.6000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: -10.0180862296
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', None, 'left'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': -10.018086229608194, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.0180862296, Q[state]-after: {'forward': -5.009043114804097, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'right', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.7782 >= 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: 2.40023966934
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 2.4002396693447725, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 0.9436019172148393}, action: left, reward: 2.40023966934, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 1.671920793279806}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.40)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.5131 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -40.3037860783
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', 'right'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': -40.30378607828359, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -40.3037860783, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -20.151893039141797}
Agent previous state: ('forward', 'red', 'forward', 'left', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.30)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.0499 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 2.11874528201
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 14, 't': 6, 'action': None, 'reward': 2.1187452820077066, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.8842363913102145, 'left': -7.58113411496795}, action: None, reward: 2.11874528201, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 2.0014908366589603, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.3653 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 1.06928737566
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': None, 'reward': 1.0692873756565464, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 2.0014908366589603, 'left': -7.58113411496795}, action: None, reward: 1.06928737566, Q[state]-after: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.5353891061577534, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.2044 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: forward, reward: -10.3586344253
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': -10.358634425314973, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.30206218445040467, None: 1.5353891061577534, 'left': -7.58113411496795}, action: forward, reward: -10.3586344253, Q[state]-after: {'forward': -5.1793172126574865, 'right': 0.30206218445040467, None: 1.5353891061577534, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.36)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.4162 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: -4.50142586841
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': -4.501425868405288, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.920339332042425, 'left': 0.5286463685931022}, action: None, reward: -4.50142586841, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.5286463685931022}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.8754 >= 0.6000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: 0.953690413715
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 10, 't': 10, 'action': 'left', 'reward': 0.9536904137149093, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.5286463685931022}, action: left, reward: 0.953690413715, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.7411683911540057}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.95)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.3767 < 0.6000
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: right, reward: 1.39922227541
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 1.3992222754099433, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.85419632413833, None: 0.0, 'left': 0.0}, action: right, reward: 1.39922227541, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.6267092997741366, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.40)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.6263 >= 0.6000
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: right, reward: 0.518287696557
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.5182876965573657, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'left', None), Q[state]-before: {'forward': -4.675016685876307, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.518287696557, Q[state]-after: {'forward': -4.675016685876307, 'right': 0.25914384827868286, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent drove right instead of forward. (rewarded 0.52)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.1945 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: right, reward: 0.119326848491
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'forward'), 'deadline': 7, 't': 13, 'action': 'right', 'reward': 0.11932684849120145, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.119326848491, Q[state]-after: {'forward': 0.0, 'right': 0.05966342424560073, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', None, 'forward')
Agent drove right instead of left. (rewarded 0.12)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.2578 < 0.6000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: forward, reward: -40.5996866726
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', None), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -40.599686672590835, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', 'forward', None), Q[state]-before: {'forward': -19.71710108977628, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.5996866726, Q[state]-after: {'forward': -30.158393881183557, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.60)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.8613 >= 0.6000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: forward, reward: -10.0104031521
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -10.010403152071042, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.0104031521, Q[state]-after: {'forward': -5.005201576035521, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -10.01)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.8028 >= 0.6000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: right, reward: 2.07402559607
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'right'), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 2.074025596066564, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.5441264685155558, None: 0.0, 'left': 0.0}, action: right, reward: 2.07402559607, Q[state]-after: {'forward': 0.0, 'right': 1.3090760322910597, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', 'right')
Agent followed the waypoint right. (rewarded 2.07)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.0518 < 0.6000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: left, reward: -9.44379425072
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': -9.443794250719177, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6328475878682835, 'left': -8.516253022598622}, action: left, reward: -9.44379425072, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6328475878682835, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.44)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.4752 < 0.6000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: None, reward: 2.04997963522
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': 2.0499796352205726, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6328475878682835, 'left': -8.980023636658899}, action: None, reward: 2.04997963522, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.8414136115444282, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.8129 >= 0.6000
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: right, reward: 0.524628659325
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 0.5246286593251539, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.6267092997741366, None: 0.0, 'left': 0.0}, action: right, reward: 0.524628659325, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.0756689795496452, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 0.52)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 9
\-------------------------

Environment.reset(): Trial set up with start = (6, 6), destination = (1, 2), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.1483 < 0.5500
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: left, reward: -39.7502728272
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', 'right'), 'deadline': 25, 't': 0, 'action': 'left', 'reward': -39.75027282722515, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -39.7502728272, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.875136413612577}
Agent previous state: ('right', 'red', 'forward', 'left', 'right')
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.2650 < 0.5500
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: -10.2119328768
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -10.211932876802003, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -4.52670615514721, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.2119328768, Q[state]-after: {'forward': -7.369319515974606, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -10.21)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6433 >= 0.5500
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: 2.08518310802
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 2.0851831080186356, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.2048872185834343, None: 0.0, 'left': 0.0}, action: right, reward: 2.08518310802, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.645035163301035, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.09)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.6428 >= 0.5500
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: -9.59636659096
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', 'right'), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': -9.596366590960656, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.59636659096, Q[state]-after: {'forward': -4.798183295480328, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -9.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.6198 >= 0.5500
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: None, reward: 2.23958494226
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.239584942264338, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -5.1793172126574865, 'right': 0.30206218445040467, None: 1.5353891061577534, 'left': -7.58113411496795}, action: None, reward: 2.23958494226, Q[state]-after: {'forward': -5.1793172126574865, 'right': 0.30206218445040467, None: 1.8874870242110457, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.1254 < 0.5500
Environment.act() [POST]: location: (7, 5), heading: (0, -1), action: left, reward: 0.205823137023
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 0.20582313702300337, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.7411683911540057}, action: left, reward: 0.205823137023, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.4734957640885045}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.21)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.2318 < 0.5500
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: left, reward: 1.82232605331
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 19, 't': 6, 'action': 'left', 'reward': 1.8223260533077132, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 1.82232605331, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9111630266538566}
Agent previous state: ('right', 'green', 'right', None, None)
Agent drove left instead of right. (rewarded 1.82)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.8822 >= 0.5500
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: right, reward: 0.771825826685
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 0.7718258266849667, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', None), Q[state]-before: {'forward': -7.426550813474842, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.771825826685, Q[state]-after: {'forward': -7.426550813474842, 'right': 0.38591291334248334, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', None)
Agent drove right instead of left. (rewarded 0.77)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.0348 < 0.5500
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: forward, reward: 0.273553448414
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'left', None), 'deadline': 17, 't': 8, 'action': 'forward', 'reward': 0.273553448413823, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.273553448414, Q[state]-after: {'forward': 0.1367767242069115, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'left', None)
Agent drove forward instead of right. (rewarded 0.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.0766 < 0.5500
Environment.act() [POST]: location: (5, 3), heading: (-1, 0), action: left, reward: 1.09047426376
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 1.090474263763187, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.0756689795496452, None: 0.0, 'left': 0.0}, action: left, reward: 1.09047426376, Q[state]-after: {'forward': 0.09117761271475183, 'right': 1.0756689795496452, None: 0.0, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent drove left instead of right. (rewarded 1.09)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.4192 < 0.5500
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: forward, reward: 1.2780436148
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'left', 'forward'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.2780436148044951, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.2780436148, Q[state]-after: {'forward': 0.6390218074022476, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'left', 'forward')
Agent drove forward instead of right. (rewarded 1.28)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.4810 < 0.5500
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: left, reward: -10.3065654685
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', 'left', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': -10.306565468492794, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'left', None), Q[state]-before: {'forward': -4.675016685876307, 'right': 0.25914384827868286, None: 0.0, 'left': 0.0}, action: left, reward: -10.3065654685, Q[state]-after: {'forward': -4.675016685876307, 'right': 0.25914384827868286, None: 0.0, 'left': -5.153282734246397}
Agent previous state: ('forward', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -10.31)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.2593 < 0.5500
Environment.act() [POST]: location: (4, 3), heading: (-1, 0), action: forward, reward: -9.71324029997
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -9.71324029996713, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -5.1793172126574865, 'right': 0.30206218445040467, None: 1.8874870242110457, 'left': -7.58113411496795}, action: forward, reward: -9.71324029997, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8874870242110457, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -9.71)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.9078 >= 0.5500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: left, reward: 0.73032833289
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 0.7303283328904551, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.4734957640885045}, action: left, reward: 0.73032833289, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.6019120484894798}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.73)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.8568 >= 0.5500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: 2.17352604793
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 2.173526047928603, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.8414136115444282, 'left': -8.980023636658899}, action: None, reward: 2.17352604793, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 2.0074698297365154, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.7745 >= 0.5500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: None, reward: 1.29041250081
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 10, 't': 15, 'action': None, 'reward': 1.2904125008088891, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 2.0074698297365154, 'left': -8.980023636658899}, action: None, reward: 1.29041250081, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6489411652727024, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.5510 >= 0.5500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: forward, reward: -10.502697085
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -10.502697084987352, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -6.954336507814753}, action: forward, reward: -10.502697085, Q[state]-after: {'forward': -5.251348542493676, 'right': 0.0, None: 0.0, 'left': -6.954336507814753}
Agent previous state: ('right', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.50)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.1496 < 0.5500
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: forward, reward: 0.366489020765
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': 0.36648902076523093, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.09117761271475183, 'right': 1.0756689795496452, None: 0.0, 'left': 0.5452371318815935}, action: forward, reward: 0.366489020765, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.0756689795496452, None: 0.0, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent drove forward instead of right. (rewarded 0.37)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.5016 < 0.5500
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: forward, reward: -10.8808174423
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': -10.880817442294498, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.8808174423, Q[state]-after: {'forward': -5.440408721147249, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent attempted driving forward through a red light. (rewarded -10.88)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.0662 < 0.5500
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: left, reward: -20.1750427411
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 6, 't': 19, 'action': 'left', 'reward': -20.175042741140462, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -20.1750427411, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.087521370570231}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.18)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.2583 < 0.5500
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: forward, reward: -0.069989451838
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, 'forward'), 'deadline': 5, 't': 20, 'action': 'forward', 'reward': -0.06998945183803085, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.087521370570231}, action: forward, reward: -0.069989451838, Q[state]-after: {'forward': -0.034994725919015424, 'right': 0.0, None: 0.0, 'left': -10.087521370570231}
Agent previous state: ('right', 'green', 'left', None, 'forward')
Agent drove forward instead of right. (rewarded -0.07)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.8317 >= 0.5500
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 2.27309958668
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.27309958667872, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.2161097793106928, 'left': 0.0}, action: None, reward: 2.27309958668, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.7446046829947064, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 2.27)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.5481 < 0.5500
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 2.10168791934
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 3, 't': 22, 'action': None, 'reward': 2.1016879193402556, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.645035163301035, None: 0.0, 'left': 0.0}, action: None, reward: 2.10168791934, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.645035163301035, None: 1.0508439596701278, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.10)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.7407 >= 0.5500
Environment.act() [POST]: location: (4, 7), heading: (0, 1), action: forward, reward: 0.324701725359
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'forward'), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': 0.32470172535915576, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.324701725359, Q[state]-after: {'forward': 0.16235086267957788, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', 'forward')
Agent drove forward instead of right. (rewarded 0.32)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.3381 < 0.5500
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: right, reward: 1.18108320594
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 1.1810832059357508, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.5241758543317744, None: 1.6489411652727024, 'left': -8.980023636658899}, action: right, reward: 1.18108320594, Q[state]-after: {'forward': -7.671544801009455, 'right': 0.8526295301337625, None: 1.6489411652727024, 'left': -8.980023636658899}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.18)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Environment.reset(): Trial set up with start = (8, 6), destination = (3, 3), deadline = 30

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.8817 >= 0.5000
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: forward, reward: -10.8961684214
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'left'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -10.896168421437585, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.8961684214, Q[state]-after: {'forward': -5.448084210718792, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -10.90)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.7280 >= 0.5000
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: 1.84206853782
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 1.8420685378172248, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.359632048898173, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 1.84206853782, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.84)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.1187 < 0.5000
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: 1.38857554568
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 28, 't': 2, 'action': 'right', 'reward': 1.3885755456779205, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 0.3894157178480324, None: 1.4715105614053559, 'left': 0.0}, action: right, reward: 1.38857554568, Q[state]-after: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 1.4715105614053559, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.39)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9339 >= 0.5000
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: right, reward: 1.79521854327
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 27, 't': 3, 'action': 'right', 'reward': 1.7952185432722327, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.658791584185761, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.79521854327, Q[state]-after: {'forward': -4.658791584185761, 'right': 0.8976092716361164, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.80)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.2992 < 0.5000
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: None, reward: -5.12987086316
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'left', 'forward'), 'deadline': 26, 't': 4, 'action': None, 'reward': -5.129870863160563, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.12987086316, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.5649354315802815, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'left', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.13)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.3782 < 0.5000
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: left, reward: -9.51559689742
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', None), 'deadline': 25, 't': 5, 'action': 'left', 'reward': -9.515596897421233, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'left', None), Q[state]-before: {'forward': -7.519666614260128, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -9.51559689742, Q[state]-after: {'forward': -7.519666614260128, 'right': 0.0, None: 0.0, 'left': -4.757798448710616}
Agent previous state: ('right', 'red', 'left', 'left', None)
Agent attempted driving left through a red light. (rewarded -9.52)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.3293 < 0.5000
Environment.act() [POST]: location: (8, 6), heading: (1, 0), action: right, reward: 1.51557345604
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 24, 't': 6, 'action': 'right', 'reward': 1.5155734560388385, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 0.8526295301337625, None: 1.6489411652727024, 'left': -8.980023636658899}, action: right, reward: 1.51557345604, Q[state]-after: {'forward': -7.671544801009455, 'right': 1.1841014930863005, None: 1.6489411652727024, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.52)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.2217 < 0.5000
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: right, reward: 1.42204622322
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 23, 't': 7, 'action': 'right', 'reward': 1.422046223215875, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.42204622322, Q[state]-after: {'forward': 0.0, 'right': 0.7110231116079375, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 1.42)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.5696 >= 0.5000
Environment.act() [POST]: location: (8, 2), heading: (0, 1), action: forward, reward: 1.89624494514
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', None), 'deadline': 22, 't': 8, 'action': 'forward', 'reward': 1.8962449451387147, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.89624494514, Q[state]-after: {'forward': 0.9481224725693573, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'left', None)
Agent drove forward instead of left. (rewarded 1.90)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.8768 >= 0.5000
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: right, reward: 1.88219271565
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 21, 't': 9, 'action': 'right', 'reward': 1.8821927156538587, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.260260184728557, 'right': 0.3126209168830425, None: 0.0, 'left': 0.0}, action: right, reward: 1.88219271565, Q[state]-after: {'forward': 0.260260184728557, 'right': 1.0974068162684505, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 1.88)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.3638 < 0.5000
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: right, reward: 1.43246070397
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 20, 't': 10, 'action': 'right', 'reward': 1.4324607039691997, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.11833293834383563, 'right': 0.0, None: 0.0, 'left': 0.9699807023994161}, action: right, reward: 1.43246070397, Q[state]-after: {'forward': 0.11833293834383563, 'right': 0.7162303519845998, None: 0.0, 'left': 0.9699807023994161}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent drove right instead of left. (rewarded 1.43)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.2320 < 0.5000
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: None, reward: -5.69415987256
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 19, 't': 11, 'action': None, 'reward': -5.694159872558748, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.69415987256, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.847079936279374, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.69)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.9271 >= 0.5000
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: 0.551676749327
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 18, 't': 12, 'action': 'left', 'reward': 0.5516767493274789, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.7675703277160356}, action: left, reward: 0.551676749327, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.6596235385217573}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 0.55)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.6491 >= 0.5000
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: forward, reward: 2.01374354718
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 17, 't': 13, 'action': 'forward', 'reward': 2.0137435471801317, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.01374354718, Q[state]-after: {'forward': 1.0068717735900659, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent followed the waypoint forward. (rewarded 2.01)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.9194 >= 0.5000
Environment.act() [POST]: location: (5, 7), heading: (-1, 0), action: forward, reward: -10.4638743959
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 16, 't': 14, 'action': 'forward', 'reward': -10.46387439594864, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.4638743959, Q[state]-after: {'forward': -5.23193719797432, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent attempted driving forward through a red light. (rewarded -10.46)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.0474 < 0.5000
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: forward, reward: 0.841342548218
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 15, 't': 15, 'action': 'forward', 'reward': 0.8413425482177952, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.841342548218, Q[state]-after: {'forward': 0.4206712741088976, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent followed the waypoint forward. (rewarded 0.84)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.4067 < 0.5000
Environment.act() [POST]: location: (4, 2), heading: (0, 1), action: left, reward: 1.71612443634
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 14, 't': 16, 'action': 'left', 'reward': 1.7161244363407888, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.6652564278720243}, action: left, reward: 1.71612443634, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 1.1906904321064067}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.72)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.2482 < 0.5000
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: left, reward: -0.195180663652
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 13, 't': 17, 'action': 'left', 'reward': -0.19518066365151188, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -0.195180663652, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent drove left instead of right. (rewarded -0.20)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent decided to explore 0.3969 < 0.5000
Environment.act() [POST]: location: (5, 2), heading: (1, 0), action: forward, reward: -10.092453624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -10.092453624000111, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -7.671544801009455, 'right': 1.1841014930863005, None: 1.6489411652727024, 'left': -8.980023636658899}, action: forward, reward: -10.092453624, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.1841014930863005, None: 1.6489411652727024, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.09)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.3896 < 0.5000
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: right, reward: 2.19191132358
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 19, 'action': 'right', 'reward': 2.191911323582335, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.1841014930863005, None: 1.6489411652727024, 'left': -8.980023636658899}, action: right, reward: 2.19191132358, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.6880064083343176, None: 1.6489411652727024, 'left': -8.980023636658899}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.19)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent decided to explore 0.3022 < 0.5000
Environment.act() [POST]: location: (5, 3), heading: (0, 1), action: None, reward: -5.10834821077
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', 'right', None), 'deadline': 10, 't': 20, 'action': None, 'reward': -5.108348210774263, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.10834821077, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.5541741053871316, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'right', None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.8570 >= 0.5000
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: left, reward: 0.497947048379
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 9, 't': 21, 'action': 'left', 'reward': 0.4979470483794918, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9111630266538566}, action: left, reward: 0.497947048379, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.7045550375166743}
Agent previous state: ('right', 'green', 'right', None, None)
Agent drove left instead of right. (rewarded 0.50)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.2657 < 0.5000
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: None, reward: -5.65650147434
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 8, 't': 22, 'action': None, 'reward': -5.656501474335527, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.0756689795496452, None: 0.0, 'left': 0.5452371318815935}, action: None, reward: -5.65650147434, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.0756689795496452, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.66)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.9716 >= 0.5000
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: 1.1685108953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 23, 'action': 'right', 'reward': 1.1685108953019743, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.0756689795496452, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.1685108953, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.1220899374258098, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.17)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.8457 >= 0.5000
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: right, reward: 1.89300325902
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'left'), 'deadline': 6, 't': 24, 'action': 'right', 'reward': 1.8930032590161772, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.7602965565274709, None: 0.0, 'left': 0.0}, action: right, reward: 1.89300325902, Q[state]-after: {'forward': 0.0, 'right': 1.3266499077718241, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', 'left')
Agent followed the waypoint right. (rewarded 1.89)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
Agent selected maxQ 0.5947 >= 0.5000
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: forward, reward: 2.1279936653
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 5, 't': 25, 'action': 'forward', 'reward': 2.1279936652969544, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}, action: forward, reward: 2.1279936653, Q[state]-after: {'forward': 1.0639968326484772, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 2.13)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
Agent decided to explore 0.3252 < 0.5000
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: None, reward: 1.26420297339
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 26, 'action': None, 'reward': 1.2642029733925657, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8874870242110457, 'left': -7.58113411496795}, action: None, reward: 1.26420297339, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -7.58113411496795}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
Agent decided to explore 0.0960 < 0.5000
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: forward, reward: -9.06032041228
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -9.06032041228102, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.06032041228, Q[state]-after: {'forward': -4.53016020614051, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light. (rewarded -9.06)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
Agent decided to explore 0.0275 < 0.5000
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: left, reward: -10.3054266755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 2, 't': 28, 'action': 'left', 'reward': -10.305426675454223, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': -4.53016020614051, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -10.3054266755, Q[state]-after: {'forward': -4.53016020614051, 'right': 0.0, None: 0.0, 'left': -5.1527133377271115}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent attempted driving left through a red light. (rewarded -10.31)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
Agent decided to explore 0.2804 < 0.5000
Environment.act() [POST]: location: (4, 4), heading: (-1, 0), action: left, reward: -20.401417602
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 3, 'light': 'green', 'state': ('forward', 'green', None, 'left', 'forward'), 'deadline': 1, 't': 29, 'action': 'left', 'reward': -20.401417601952588, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -20.401417602, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.200708800976294}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', None, 'left', 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.40)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 11
\-------------------------

Environment.reset(): Trial set up with start = (2, 3), destination = (7, 2), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.4191 < 0.4500
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: -4.47506133478
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'left', None, 'right'), 'deadline': 20, 't': 0, 'action': None, 'reward': -4.475061334782929, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -4.47506133478, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.2375306673914643, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -4.48)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.3285 < 0.4500
Environment.act() [POST]: location: (2, 3), heading: (0, 1), action: None, reward: -5.80267974297
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': -5.802679742970169, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -3.6374799457212106, 'left': 0.8854462406807438}, action: None, reward: -5.80267974297, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.8854462406807438}
Agent previous state: ('right', 'green', 'left', None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent decided to explore 0.0385 < 0.4500
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: right, reward: 1.33189325932
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 1.331893259323805, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.7045550375166743}, action: right, reward: 1.33189325932, Q[state]-after: {'forward': 0.0, 'right': 0.6659466296619025, None: 0.0, 'left': 0.7045550375166743}
Agent previous state: ('right', 'green', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.2860 < 0.4500
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: left, reward: -10.3634606829
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': -10.3634606828694, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -7.58113411496795}, action: left, reward: -10.3634606829, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -8.972297398918675}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.36)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.9575 >= 0.4500
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: None, reward: 1.8284504376
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 16, 't': 4, 'action': None, 'reward': 1.828450437603499, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.053998905939289, 'left': 0.0}, action: None, reward: 1.8284504376, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.441224671771394, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.5567 >= 0.4500
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: forward, reward: 2.63134238032
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 2.631342380316809, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.6935317604564553, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.63134238032, Q[state]-after: {'forward': 1.662437070386632, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent followed the waypoint forward. (rewarded 2.63)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.2047 < 0.4500
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: 0.332621742855
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 0.33262174285478485, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.7110231116079375, None: 0.0, 'left': 0.0}, action: left, reward: 0.332621742855, Q[state]-after: {'forward': 0.0, 'right': 0.7110231116079375, None: 0.0, 'left': 0.16631087142739243}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove left instead of forward. (rewarded 0.33)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.4295 < 0.4500
Environment.act() [POST]: location: (8, 4), heading: (0, 1), action: left, reward: -10.9546651317
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 13, 't': 7, 'action': 'left', 'reward': -10.954665131722386, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.6880064083343176, None: 1.6489411652727024, 'left': -8.980023636658899}, action: left, reward: -10.9546651317, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.6880064083343176, None: 1.6489411652727024, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -10.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.8630 >= 0.4500
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: right, reward: 0.961075337241
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 0.9610753372410035, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.6880064083343176, None: 1.6489411652727024, 'left': -9.967344384190643}, action: right, reward: 0.961075337241, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.3245408727876606, None: 1.6489411652727024, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 0.96)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.6698 >= 0.4500
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: forward, reward: -0.0222898510555
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 11, 't': 9, 'action': 'forward', 'reward': -0.02228985105554737, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.0222898510555, Q[state]-after: {'forward': -0.011144925527773686, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent drove forward instead of right. (rewarded -0.02)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.6430 >= 0.4500
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: 0.44935510489
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', 'forward'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 0.44935510489045616, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', 'forward'), Q[state]-before: {'forward': 0.16235086267957788, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.44935510489, Q[state]-after: {'forward': 0.305852983785017, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', 'forward')
Agent drove forward instead of right. (rewarded 0.45)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.7959 >= 0.4500
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: right, reward: 0.798528181952
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 0.7985281819521368, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', None), Q[state]-before: {'forward': -5.251348542493676, 'right': 0.0, None: 0.0, 'left': -6.954336507814753}, action: right, reward: 0.798528181952, Q[state]-after: {'forward': -5.251348542493676, 'right': 0.3992640909760684, None: 0.0, 'left': -6.954336507814753}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 0.80)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.9053 >= 0.4500
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: forward, reward: 0.975659876354
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': 0.9756598763542081, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.41487917028895505, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.975659876354, Q[state]-after: {'forward': 0.6952695233215815, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent drove forward instead of right. (rewarded 0.98)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.1792 < 0.4500
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: None, reward: 0.118302776087
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 7, 't': 13, 'action': None, 'reward': 0.11830277608697426, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -0.09759033182575594}, action: None, reward: 0.118302776087, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.05915138804348713, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.12)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.7086 >= 0.4500
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: 0.661714070865
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 0.6617140708650092, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.661714070865, Q[state]-after: {'forward': 0.3308570354325046, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.66)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.8045 >= 0.4500
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: None, reward: 1.53048397266
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 5, 't': 15, 'action': None, 'reward': 1.5304839726599533, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 0.0, 'left': 0.0}, action: None, reward: 1.53048397266, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 0.7652419863299766, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 1.53)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.4042 < 0.4500
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: right, reward: 1.67141772985
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 1.6714177298501958, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.67141772985, Q[state]-after: {'forward': 0.0, 'right': 0.8357088649250979, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent followed the waypoint right. (rewarded 1.67)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.9280 >= 0.4500
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: forward, reward: 0.872583855456
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': 0.872583855455709, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, 'forward'), Q[state]-before: {'forward': 1.662437070386632, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.872583855456, Q[state]-after: {'forward': 1.2675104629211704, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent followed the waypoint forward. (rewarded 0.87)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.8936 >= 0.4500
Environment.act() [POST]: location: (7, 6), heading: (0, -1), action: left, reward: -0.457725293953
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 2, 't': 18, 'action': 'left', 'reward': -0.4577252939531662, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.6596235385217573}, action: left, reward: -0.457725293953, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.10094912228429553}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded -0.46)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.0755 < 0.4500
Environment.act() [POST]: location: (6, 6), heading: (-1, 0), action: left, reward: 0.951688457987
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': 0.9516884579869976, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', None), Q[state]-before: {'forward': 0.869936733813745, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: 0.951688457987, Q[state]-after: {'forward': 0.869936733813745, 'right': 0.0, None: 0.0, 'left': 0.4758442289934988}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove left instead of right. (rewarded 0.95)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 12
\-------------------------

Environment.reset(): Trial set up with start = (5, 5), destination = (7, 2), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.3962 < 0.4000
Environment.act() [POST]: location: (5, 4), heading: (0, -1), action: right, reward: 1.49958686002
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'left'), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 1.4995868600172293, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.3332099408266512, 'left': 0.0}, action: right, reward: 1.49958686002, Q[state]-after: {'forward': 0.0, 'right': 0.7497934300086146, None: 0.3332099408266512, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'right', 'left')
Agent drove right instead of left. (rewarded 1.50)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.0174 < 0.4000
Environment.act() [POST]: location: (5, 3), heading: (0, -1), action: forward, reward: 0.481820968419
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 0.4818209684189585, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', None), Q[state]-before: {'forward': 0.41014210241385907, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.481820968419, Q[state]-after: {'forward': 0.4459815354164088, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove forward instead of right. (rewarded 0.48)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.7619 >= 0.4000
Environment.act() [POST]: location: (6, 3), heading: (1, 0), action: right, reward: 2.54687481466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 23, 't': 2, 'action': 'right', 'reward': 2.5468748146635094, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.645035163301035, None: 1.0508439596701278, 'left': 0.0}, action: right, reward: 2.54687481466, Q[state]-after: {'forward': -4.895273665926438, 'right': 2.095954988982272, None: 1.0508439596701278, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.55)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.6257 >= 0.4000
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: right, reward: 0.733918736945
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.7339187369448104, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.8265515035756898, None: 0.0, 'left': 0.0}, action: right, reward: 0.733918736945, Q[state]-after: {'forward': 0.0, 'right': 0.7802351202602501, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 0.73)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.2271 < 0.4000
Environment.act() [POST]: location: (6, 4), heading: (0, 1), action: None, reward: 1.87584626371
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.875846263714106, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 1.4715105614053559, 'left': 0.0}, action: None, reward: 1.87584626371, Q[state]-after: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 1.673678412559731, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent decided to explore 0.2240 < 0.4000
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: left, reward: 2.81289000809
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 2.812890008093116, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 1.671920793279806}, action: left, reward: 2.81289000809, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.2424054006864607}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.81)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.6828 >= 0.4000
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: None, reward: 2.35131252152
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 19, 't': 6, 'action': None, 'reward': 2.351312521517905, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 1.673678412559731, 'left': 0.0}, action: None, reward: 2.35131252152, Q[state]-after: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 2.0124954670388178, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.2850 < 0.4000
Environment.act() [POST]: location: (7, 5), heading: (0, 1), action: right, reward: 1.35492507828
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.3549250782840485, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 0.8889956317629765, None: 2.0124954670388178, 'left': 0.0}, action: right, reward: 1.35492507828, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.1219603550235124, None: 2.0124954670388178, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.35)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.9892 >= 0.4000
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: right, reward: 1.25822007236
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.258220072364967, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.7110231116079375, None: 0.0, 'left': 0.16631087142739243}, action: right, reward: 1.25822007236, Q[state]-after: {'forward': 0.0, 'right': 0.9846215919864523, None: 0.0, 'left': 0.16631087142739243}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 1.26)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.1021 < 0.4000
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: right, reward: 1.68493027909
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 1.6849302790908653, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.1219603550235124, None: 2.0124954670388178, 'left': 0.0}, action: right, reward: 1.68493027909, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 2.0124954670388178, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.3942 < 0.4000
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: right, reward: 1.83205060673
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.8320506067271782, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.6659466296619025, None: 0.0, 'left': 0.7045550375166743}, action: right, reward: 1.83205060673, Q[state]-after: {'forward': 0.0, 'right': 1.2489986181945403, None: 0.0, 'left': 0.7045550375166743}
Agent previous state: ('right', 'green', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.83)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.5232 >= 0.4000
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: forward, reward: 1.42360332665
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.423603326646699, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.42360332665, Q[state]-after: {'forward': 0.7118016633233495, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 1.42)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.5290 >= 0.4000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: right, reward: 0.563650832378
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 13, 't': 12, 'action': 'right', 'reward': 0.5636508323777938, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -4.581629062157533, 'right': 0.9872521074995722, None: 0.8691367301007654, 'left': -8.373439975220673}, action: right, reward: 0.563650832378, Q[state]-after: {'forward': -4.581629062157533, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -8.373439975220673}
Agent previous state: ('left', 'red', None, None, None)
Agent drove right instead of left. (rewarded 0.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.8809 >= 0.4000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: None, reward: 2.4917349963
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 12, 't': 13, 'action': None, 'reward': 2.491734996303353, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.7446046829947064, 'left': 0.0}, action: None, reward: 2.4917349963, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 2.1181698396490294, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 2.49)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.4588 >= 0.4000
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 0.922845126312
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 0.9228451263120647, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 2.095954988982272, None: 1.0508439596701278, 'left': 0.0}, action: right, reward: 0.922845126312, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.0508439596701278, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.92)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.6170 >= 0.4000
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: forward, reward: -9.69128372905
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': -9.691283729048873, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.69128372905, Q[state]-after: {'forward': -4.845641864524437, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.69)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.0060 < 0.4000
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: left, reward: -9.93384155881
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': -9.933841558808261, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -4.581629062157533, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -8.373439975220673}, action: left, reward: -9.93384155881, Q[state]-after: {'forward': -4.581629062157533, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.93)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.4743 >= 0.4000
Environment.act() [POST]: location: (6, 5), heading: (-1, 0), action: forward, reward: 1.40538898119
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'right'), 'deadline': 8, 't': 17, 'action': 'forward', 'reward': 1.4053889811916138, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.8065214326506194, 'left': -10.341820645821178}, action: forward, reward: 1.40538898119, Q[state]-after: {'forward': 0.7026944905958069, 'right': 0.0, None: -2.8065214326506194, 'left': -10.341820645821178}
Agent previous state: ('left', 'green', None, None, 'right')
Agent drove forward instead of left. (rewarded 1.41)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.5836 >= 0.4000
Environment.act() [POST]: location: (6, 4), heading: (0, -1), action: right, reward: 0.960819797959
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'right', None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 0.9608197979591864, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'right', None), Q[state]-before: {'forward': -4.845641864524437, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.960819797959, Q[state]-after: {'forward': -4.845641864524437, 'right': 0.4804098989795932, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'right', None)
Agent drove right instead of left. (rewarded 0.96)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent decided to explore 0.2355 < 0.4000
Environment.act() [POST]: location: (7, 4), heading: (1, 0), action: right, reward: 1.18879833197
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 1.188798331965681, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.3245408727876606, None: 1.6489411652727024, 'left': -9.967344384190643}, action: right, reward: 1.18879833197, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.2566696023766708, None: 1.6489411652727024, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent selected maxQ 0.7685 >= 0.4000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: left, reward: 1.60661359121
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 5, 't': 20, 'action': 'left', 'reward': 1.6066135912079067, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.380833987287979}, action: left, reward: 1.60661359121, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9937237892479429}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent followed the waypoint left. (rewarded 1.61)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent decided to explore 0.3823 < 0.4000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: forward, reward: -39.7483873925
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'forward', None), 'deadline': 4, 't': 21, 'action': 'forward', 'reward': -39.7483873925206, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.7483873925, Q[state]-after: {'forward': -19.8741936962603, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.75)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent decided to explore 0.1595 < 0.4000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: None, reward: 0.863679005483
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'right'), 'deadline': 3, 't': 22, 'action': None, 'reward': 0.863679005482975, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: 0.863679005483, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.4318395027414875, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', 'right')
Agent properly idled at a red light. (rewarded 0.86)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent decided to explore 0.2165 < 0.4000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: left, reward: -9.25842885927
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 2, 't': 23, 'action': 'left', 'reward': -9.258428859266477, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -8.972297398918675}, action: left, reward: -9.25842885927, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent attempted driving left through a red light. (rewarded -9.26)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent decided to explore 0.1717 < 0.4000
Environment.act() [POST]: location: (7, 3), heading: (0, -1), action: left, reward: -40.9153200448
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', None, None), 'deadline': 1, 't': 24, 'action': 'left', 'reward': -40.915320044772876, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.885852272314494}, action: left, reward: -40.9153200448, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -30.400586158543685}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'forward', None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 13
\-------------------------

Environment.reset(): Trial set up with start = (4, 3), destination = (1, 5), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.9731 >= 0.3500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: forward, reward: -40.0300571873
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', 'forward'), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': -40.03005718727145, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.0300571873, Q[state]-after: {'forward': -20.015028593635726, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.03)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.2246 < 0.3500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 2.48843691287
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 24, 't': 1, 'action': None, 'reward': 2.4884369128744757, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.0508439596701278, 'left': 0.0}, action: None, reward: 2.48843691287, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.7696404362723017, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.49)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.8877 >= 0.3500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: 2.16128349424
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 23, 't': 2, 'action': None, 'reward': 2.1612834942351933, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.7696404362723017, 'left': 0.0}, action: None, reward: 2.16128349424, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.9654619652537475, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.16)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent decided to explore 0.0518 < 0.3500
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: None, reward: -5.22047993015
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 22, 't': 3, 'action': None, 'reward': -5.2204799301463645, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.300089730189393, None: -2.5270375447073965, 'left': 0.0}, action: None, reward: -5.22047993015, Q[state]-after: {'forward': 0.0, 'right': 1.300089730189393, None: -3.8737587374268805, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent idled at a green light with no oncoming traffic. (rewarded -5.22)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.9125 >= 0.3500
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: right, reward: 2.46712455147
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 21, 't': 4, 'action': 'right', 'reward': 2.467124551471601, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.1220899374258098, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 2.46712455147, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7946072444487053, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.47)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9846 >= 0.3500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: right, reward: 1.36014750141
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'right', 'reward': 1.3601475014113993, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7946072444487053, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.36014750141, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.5773773729300524, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent decided to explore 0.2540 < 0.3500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: -5.54639178158
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, 'right'), 'deadline': 19, 't': 6, 'action': None, 'reward': -5.54639178158326, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -5.54639178158, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.77319589079163, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.7878 >= 0.3500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: 2.05092345279
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 18, 't': 7, 'action': None, 'reward': 2.0509234527946787, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.5758449988018057, 'left': -9.115363129092575}, action: None, reward: 2.05092345279, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8133842257982422, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent decided to explore 0.2093 < 0.3500
Environment.act() [POST]: location: (3, 4), heading: (-1, 0), action: None, reward: 1.31310017864
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 17, 't': 8, 'action': None, 'reward': 1.3131001786360688, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.6025619083949016, None: 0.7057009205178898, 'left': 0.0}, action: None, reward: 1.31310017864, Q[state]-after: {'forward': 0.0, 'right': 0.6025619083949016, None: 1.0094005495769793, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.31)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.1362 < 0.3500
Environment.act() [POST]: location: (3, 5), heading: (0, 1), action: left, reward: -0.0100553890509
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': -0.01005538905094605, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 1.1906904321064067}, action: left, reward: -0.0100553890509, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.5903175215277303}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded -0.01)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.4680 >= 0.3500
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: right, reward: 1.54796674568
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, None), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.5479667456791288, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', None, None), Q[state]-before: {'forward': -0.011144925527773686, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.54796674568, Q[state]-after: {'forward': -0.011144925527773686, 'right': 0.7739833728395644, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', None, None)
Agent followed the waypoint right. (rewarded 1.55)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.8216 >= 0.3500
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: forward, reward: 1.86480193697
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'left', None), 'deadline': 14, 't': 11, 'action': 'forward', 'reward': 1.86480193696775, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.86480193697, Q[state]-after: {'forward': 0.932400968483875, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', 'left', None)
Agent followed the waypoint forward. (rewarded 1.86)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 14
\-------------------------

Environment.reset(): Trial set up with start = (6, 2), destination = (1, 3), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent decided to explore 0.2059 < 0.3000
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: forward, reward: -9.43688401349
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -9.436884013490504, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.6025619083949016, None: 1.0094005495769793, 'left': 0.0}, action: forward, reward: -9.43688401349, Q[state]-after: {'forward': -4.718442006745252, 'right': 0.6025619083949016, None: 1.0094005495769793, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.44)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.6762 >= 0.3000
Environment.act() [POST]: location: (6, 2), heading: (1, 0), action: None, reward: 2.50879935826
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.508799358263973, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8133842257982422, 'left': -9.115363129092575}, action: None, reward: 2.50879935826, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.1610917920311077, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6692 >= 0.3000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: right, reward: 0.449880738203
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 0.4498807382031659, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': -7.669334934958838, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.449880738203, Q[state]-after: {'forward': -7.669334934958838, 'right': 0.22494036910158294, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9873 >= 0.3000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: None, reward: 1.34808592544
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 17, 't': 3, 'action': None, 'reward': 1.3480859254389186, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 2.0124954670388178, 'left': 0.0}, action: None, reward: 1.34808592544, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 1.6802906962388682, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.3988 >= 0.3000
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: None, reward: 0.99101654624
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 16, 't': 4, 'action': None, 'reward': 0.9910165462399503, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 1.6802906962388682, 'left': 0.0}, action: None, reward: 0.99101654624, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 1.3356536212394092, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 0.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9512 >= 0.3000
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: left, reward: 2.33764802632
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 2.3376480263196653, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.2424054006864607}, action: left, reward: 2.33764802632, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.290026713503063}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.34)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.5053 >= 0.3000
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: forward, reward: -39.3082435022
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': -39.30824350218805, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.3082435022, Q[state]-after: {'forward': -19.654121751094024, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.8727 >= 0.3000
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: right, reward: 1.49458803517
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'forward'), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.4945880351666143, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': -4.53016020614051, 'right': 0.0, None: 0.0, 'left': -5.1527133377271115}, action: right, reward: 1.49458803517, Q[state]-after: {'forward': -4.53016020614051, 'right': 0.7472940175833072, None: 0.0, 'left': -5.1527133377271115}
Agent previous state: ('forward', 'red', None, 'left', 'forward')
Agent drove right instead of forward. (rewarded 1.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.6676 >= 0.3000
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: left, reward: 2.39663540353
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'left', 'reward': 2.3966354035255413, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.290026713503063}, action: left, reward: 2.39663540353, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.40)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.3012 >= 0.3000
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: None, reward: 1.61957014409
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.6195701440934909, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.1610917920311077, 'left': -9.115363129092575}, action: None, reward: 1.61957014409, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8903309680622993, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.62)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.4461 >= 0.3000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: right, reward: 0.498832537072
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.498832537072217, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': -7.669334934958838, 'right': 0.22494036910158294, None: 0.0, 'left': 0.0}, action: right, reward: 0.498832537072, Q[state]-after: {'forward': -7.669334934958838, 'right': 0.36188645308689993, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.0486 < 0.3000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: left, reward: -40.8996758665
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', None), 'deadline': 9, 't': 11, 'action': 'left', 'reward': -40.899675866475654, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', None), Q[state]-before: {'forward': -20.315592053517015, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -40.8996758665, Q[state]-after: {'forward': -20.315592053517015, 'right': 0.0, None: 0.0, 'left': -20.449837933237827}
Agent previous state: ('left', 'red', 'forward', 'left', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.90)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.9309 >= 0.3000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: right, reward: -19.9662127626
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': -19.96621276256505, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', None), Q[state]-before: {'forward': -20.315592053517015, 'right': 0.0, None: 0.0, 'left': -20.449837933237827}, action: right, reward: -19.9662127626, Q[state]-after: {'forward': -20.315592053517015, 'right': -9.983106381282525, None: 0.0, 'left': -20.449837933237827}
Agent previous state: ('left', 'red', 'forward', 'left', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.97)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.3693 >= 0.3000
Environment.act() [POST]: location: (8, 6), heading: (0, 1), action: forward, reward: 1.08033726352
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 1.0803372635243549, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', None), Q[state]-before: {'forward': 0.7458547482208362, 'right': 0.0, None: -2.1845675904383586, 'left': 0.0}, action: forward, reward: 1.08033726352, Q[state]-after: {'forward': 0.9130960058725955, 'right': 0.0, None: -2.1845675904383586, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 1.08)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.3142 >= 0.3000
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: right, reward: -0.325044409804
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'right', None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': -0.325044409803822, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.7486149261909009, None: 0.0, 'left': 0.0}, action: right, reward: -0.325044409804, Q[state]-after: {'forward': 0.0, 'right': 0.21178525819353944, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'right', None)
Agent drove right instead of left. (rewarded -0.33)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent decided to explore 0.2804 < 0.3000
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: forward, reward: -10.2180439426
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -10.218043942648379, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -4.581629062157533, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -9.153640767014467}, action: forward, reward: -10.2180439426, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent attempted driving forward through a red light. (rewarded -10.22)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.6606 >= 0.3000
Environment.act() [POST]: location: (7, 6), heading: (-1, 0), action: None, reward: 2.15870814851
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 4, 't': 16, 'action': None, 'reward': 2.1587081485125434, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 0.8691367301007654, 'left': -9.153640767014467}, action: None, reward: 2.15870814851, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5139224393066544, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.8652 >= 0.3000
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: left, reward: 0.579491539702
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'forward', None), 'deadline': 3, 't': 17, 'action': 'left', 'reward': 0.5794915397021121, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.11833293834383563, 'right': 0.7162303519845998, None: 0.0, 'left': 0.9699807023994161}, action: left, reward: 0.579491539702, Q[state]-after: {'forward': 0.11833293834383563, 'right': 0.7162303519845998, None: 0.0, 'left': 0.7747361210507642}
Agent previous state: ('left', 'green', None, 'forward', None)
Agent followed the waypoint left. (rewarded 0.58)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.4413 >= 0.3000
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: forward, reward: -40.2319545033
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'left'), 'deadline': 2, 't': 18, 'action': 'forward', 'reward': -40.23195450330864, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.2319545033, Q[state]-after: {'forward': -20.11597725165432, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'left', 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.23)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.7343 >= 0.3000
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: forward, reward: -10.8850897017
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', 'left', 'left'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': -10.885089701663091, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.8850897017, Q[state]-after: {'forward': -5.4425448508315455, 'right': 0.0, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', 'left', 'left', 'left')
Agent attempted driving forward through a red light. (rewarded -10.89)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Environment.reset(): Trial set up with start = (5, 7), destination = (7, 5), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.7891 >= 0.2500
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: forward, reward: 2.80189992117
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'right'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 2.8018999211700537, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.80189992117, Q[state]-after: {'forward': 1.4009499605850269, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, 'right')
Agent followed the waypoint forward. (rewarded 2.80)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.2247 < 0.2500
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: None, reward: -4.38337359001
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 1, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': -4.383373590010096, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.710882600223856, 'left': 0.6019120484894798}, action: None, reward: -4.38337359001, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.6019120484894798}
Agent previous state: ('forward', 'green', None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.3154 >= 0.2500
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: forward, reward: 1.16229953087
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': 1.1622995308655297, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'forward'), Q[state]-before: {'forward': 1.1091794990164636, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.16229953087, Q[state]-after: {'forward': 1.1357395149409966, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 1.16)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.5818 >= 0.2500
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: forward, reward: 0.800951952594
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 0.8009519525936538, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, None), Q[state]-before: {'forward': 0.7118016633233495, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.800951952594, Q[state]-after: {'forward': 0.7563768079585016, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 0.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent decided to explore 0.1265 < 0.2500
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: forward, reward: -39.1475129114
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', None, 'forward', 'forward'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -39.14751291136981, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.1475129114, Q[state]-after: {'forward': -19.573756455684904, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.15)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.6775 >= 0.2500
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: forward, reward: -9.85135247147
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'left', None, 'forward'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': -9.851352471472499, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.85135247147, Q[state]-after: {'forward': -4.925676235736249, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light. (rewarded -9.85)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.8542 >= 0.2500
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: 0.678793104535
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'right', 'forward'), 'deadline': 14, 't': 6, 'action': 'forward', 'reward': 0.6787931045348505, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.678793104535, Q[state]-after: {'forward': 0.33939655226742527, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'right', 'forward')
Agent drove forward instead of left. (rewarded 0.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent decided to explore 0.1423 < 0.2500
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 0.361622626988
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'left', None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 0.36162262698812797, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'left', None), Q[state]-before: {'forward': -4.562074543855394, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.361622626988, Q[state]-after: {'forward': -4.562074543855394, 'right': 0.18081131349406399, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'left', 'left', None)
Agent drove right instead of left. (rewarded 0.36)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.6922 >= 0.2500
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: None, reward: 1.70053248664
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 12, 't': 8, 'action': None, 'reward': 1.7005324866420048, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.2566696023766708, None: 1.6489411652727024, 'left': -9.967344384190643}, action: None, reward: 1.70053248664, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.2566696023766708, None: 1.6747368259573536, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.3810 >= 0.2500
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 1.61035167592
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.6103516759248315, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.61035167592, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent followed the waypoint right. (rewarded 1.61)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.5680 >= 0.2500
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: forward, reward: 2.69639166519
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', None), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 2.6963916651927384, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', None), Q[state]-before: {'forward': 0.4206712741088976, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.69639166519, Q[state]-after: {'forward': 1.5585314696508181, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', None)
Agent followed the waypoint forward. (rewarded 2.70)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent decided to explore 0.0238 < 0.2500
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: None, reward: 2.6888009016
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 9, 't': 11, 'action': None, 'reward': 2.688800901600727, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 0.0, 'left': 0.0}, action: None, reward: 2.6888009016, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 1.3444004508003635, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent decided to explore 0.1396 < 0.2500
Environment.act() [POST]: location: (7, 7), heading: (0, -1), action: right, reward: 2.17937212291
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 2.179372122911386, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.17937212291, Q[state]-after: {'forward': 0.0, 'right': 1.089686061455693, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 2.18)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.3517 >= 0.2500
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: left, reward: 0.373655269134
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 0.37365526913359626, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.6019120484894798}, action: left, reward: 0.373655269134, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.487783658811538}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.37)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent decided to explore 0.2141 < 0.2500
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 1.48159220821
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 1.4815922082147208, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.2566696023766708, None: 1.6747368259573536, 'left': -9.967344384190643}, action: right, reward: 1.48159220821, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.369130905295696, None: 1.6747368259573536, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.48)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.4079 >= 0.2500
Environment.act() [POST]: location: (6, 5), heading: (0, -1), action: forward, reward: -0.27967935025
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -0.2796793502497321, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.6952695233215815, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.27967935025, Q[state]-after: {'forward': 0.2077950865359247, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent drove forward instead of right. (rewarded -0.28)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent decided to explore 0.0598 < 0.2500
Environment.act() [POST]: location: (6, 5), heading: (0, -1), action: left, reward: -39.9680454219
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': -39.96804542190933, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: left, reward: -39.9680454219, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.984022710954665}
Agent previous state: ('right', 'red', 'forward', 'left', None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.97)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.9476 >= 0.2500
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: right, reward: 1.3870578702
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 1.3870578701998793, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -7.369319515974606, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.3870578702, Q[state]-after: {'forward': -7.369319515974606, 'right': 0.6935289350999396, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 1.39)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 16
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (4, 7), deadline = 30

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.7719 >= 0.2000
Environment.act() [POST]: location: (7, 4), heading: (0, 1), action: forward, reward: -9.88745232852
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', 'left', 'right'), 'deadline': 30, 't': 0, 'action': 'forward', 'reward': -9.887452328516623, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.88745232852, Q[state]-after: {'forward': -4.943726164258312, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -9.89)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.0938 < 0.2000
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: right, reward: 2.103249668
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 29, 't': 1, 'action': 'right', 'reward': 2.1032496679961885, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.369130905295696, None: 1.6747368259573536, 'left': -9.967344384190643}, action: right, reward: 2.103249668, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.7361902866459422, None: 1.6747368259573536, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 2.10)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6342 >= 0.2000
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: forward, reward: -40.1168614275
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', 'forward'), 'deadline': 28, 't': 2, 'action': 'forward', 'reward': -40.11686142745188, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.1168614275, Q[state]-after: {'forward': -20.05843071372594, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.12)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9226 >= 0.2000
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: forward, reward: 2.04251285958
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 27, 't': 3, 'action': 'forward', 'reward': 2.0425128595833666, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'forward'), Q[state]-before: {'forward': 1.1357395149409966, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.04251285958, Q[state]-after: {'forward': 1.5891261872621816, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 2.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.8039 >= 0.2000
Environment.act() [POST]: location: (5, 4), heading: (-1, 0), action: None, reward: 1.37157242
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 26, 't': 4, 'action': None, 'reward': 1.3715724200000314, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8903309680622993, 'left': -9.115363129092575}, action: None, reward: 1.37157242, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.6309516940311655, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.7275 >= 0.2000
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: left, reward: 1.47740635632
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 25, 't': 5, 'action': 'left', 'reward': 1.4774063563228679, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.487783658811538}, action: left, reward: 1.47740635632, Q[state]-after: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.982595007567203}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.4908 >= 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: left, reward: 1.80868179957
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 24, 't': 6, 'action': 'left', 'reward': 1.8086817995739621, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.10094912228429553}, action: left, reward: 1.80868179957, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9548154609291288}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 1.81)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.4135 >= 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 2.73341284256
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 23, 't': 7, 'action': None, 'reward': 2.733412842563424, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 1.3444004508003635, 'left': 0.0}, action: None, reward: 2.73341284256, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.0389066466818937, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.73)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.8173 >= 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 2.73799621276
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 22, 't': 8, 'action': None, 'reward': 2.7379962127562605, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.0389066466818937, 'left': 0.0}, action: None, reward: 2.73799621276, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.388451429719077, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.4219 >= 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 1.80687179708
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 21, 't': 9, 'action': None, 'reward': 1.8068717970787456, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.388451429719077, 'left': 0.0}, action: None, reward: 1.80687179708, Q[state]-after: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.097661613398911, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.81)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.0330 < 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: forward, reward: -10.8936504819
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 20, 't': 10, 'action': 'forward', 'reward': -10.893650481907699, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -5.188435284484253, 'right': 0.8051758379624158, None: 2.097661613398911, 'left': 0.0}, action: forward, reward: -10.8936504819, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.097661613398911, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.89)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.9510 >= 0.2000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: None, reward: 1.37532847214
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 19, 't': 11, 'action': None, 'reward': 1.3753284721350143, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.05915138804348713, 'left': -0.09759033182575594}, action: None, reward: 1.37532847214, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.7172399300892507, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.38)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.2272 >= 0.2000
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: forward, reward: 0.931399851759
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 18, 't': 12, 'action': 'forward', 'reward': 0.9313998517587473, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', 'left'), Q[state]-before: {'forward': 0.17454654469921677, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.931399851759, Q[state]-after: {'forward': 0.552973198228982, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent drove forward instead of right. (rewarded 0.93)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent decided to explore 0.1317 < 0.2000
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: None, reward: -4.00373549485
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'right'}, 'violation': 1, 'light': 'green', 'state': ('right', 'green', 'right', 'left', 'right'), 'deadline': 17, 't': 13, 'action': None, 'reward': -4.003735494854884, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: None, reward: -4.00373549485, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.001867747427442, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', 'left', 'right')
Agent idled at a green light with no oncoming traffic. (rewarded -4.00)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.3700 >= 0.2000
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: 0.770895475442
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'left', None), 'deadline': 16, 't': 14, 'action': 'forward', 'reward': 0.7708954754423534, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'left', None), Q[state]-before: {'forward': 0.4459815354164088, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.770895475442, Q[state]-after: {'forward': 0.608438505429381, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'left', None)
Agent drove forward instead of right. (rewarded 0.77)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.3073 >= 0.2000
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: None, reward: 1.43115083339
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 15, 't': 15, 'action': None, 'reward': 1.4311508333907779, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': -4.718442006745252, 'right': 0.6025619083949016, None: 1.0094005495769793, 'left': 0.0}, action: None, reward: 1.43115083339, Q[state]-after: {'forward': -4.718442006745252, 'right': 0.6025619083949016, None: 1.2202756914838786, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 1.43)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.3831 >= 0.2000
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: right, reward: -20.6138809342
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 14, 't': 16, 'action': 'right', 'reward': -20.613880934165735, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'left', None), Q[state]-before: {'forward': -19.654121751094024, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -20.6138809342, Q[state]-after: {'forward': -19.654121751094024, 'right': -10.306940467082867, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.61)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.0353 < 0.2000
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: forward, reward: 1.27048278927
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 13, 't': 17, 'action': 'forward', 'reward': 1.2704827892667476, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.0, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.982595007567203}, action: forward, reward: 1.27048278927, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.982595007567203}
Agent previous state: ('forward', 'green', None, None, None)
Agent followed the waypoint forward. (rewarded 1.27)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.8480 >= 0.2000
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: forward, reward: -9.06361201755
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, None, 'right'), 'deadline': 12, 't': 18, 'action': 'forward', 'reward': -9.063612017545536, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.06361201755, Q[state]-after: {'forward': -4.531806008772768, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.06)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.5490 >= 0.2000
Environment.act() [POST]: location: (1, 4), heading: (0, -1), action: left, reward: 0.561186751194
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 11, 't': 19, 'action': 'left', 'reward': 0.5611867511938925, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.982595007567203}, action: left, reward: 0.561186751194, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.7718908793805477}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.56)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent selected maxQ 0.6859 >= 0.2000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: right, reward: 1.60657873201
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 20, 'action': 'right', 'reward': 1.606578732005827, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.5773773729300524, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.60657873201, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.5919780524679397, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.61)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.3279 >= 0.2000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: 1.20732902239
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 9, 't': 21, 'action': None, 'reward': 1.2073290223905508, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.1185291699953366, 'left': 0.0}, action: None, reward: 1.20732902239, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.1629290961929437, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.21)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent selected maxQ 0.8223 >= 0.2000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: 1.20643411773
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 22, 'action': None, 'reward': 1.206434117731471, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.6309516940311655, 'left': -9.115363129092575}, action: None, reward: 1.20643411773, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.4186929058813182, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.5929 >= 0.2000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: 2.31410308474
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 23, 'action': None, 'reward': 2.3141030847412765, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.4186929058813182, 'left': -9.115363129092575}, action: None, reward: 2.31410308474, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8663979953112975, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.2620 >= 0.2000
Environment.act() [POST]: location: (2, 4), heading: (1, 0), action: None, reward: 2.40430914546
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 24, 'action': None, 'reward': 2.4043091454610863, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8663979953112975, 'left': -9.115363129092575}, action: None, reward: 2.40430914546, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.135353570386192, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.40)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Environment.step(): t = 25
Agent selected maxQ 0.6516 >= 0.2000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: left, reward: 1.31996005068
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 25, 'action': 'left', 'reward': 1.3199600506812117, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 0.7718908793805477}, action: left, reward: 1.31996005068, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0459254650308796}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.32)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Environment.step(): t = 26
Agent decided to explore 0.1010 < 0.2000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: forward, reward: 0.648836743782
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 4, 't': 26, 'action': 'forward', 'reward': 0.6488367437817115, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', None), Q[state]-before: {'forward': 0.869936733813745, 'right': 0.0, None: 0.0, 'left': 0.4758442289934988}, action: forward, reward: 0.648836743782, Q[state]-after: {'forward': 0.7593867387977282, 'right': 0.0, None: 0.0, 'left': 0.4758442289934988}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 0.65)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Environment.step(): t = 27
Agent selected maxQ 0.3322 >= 0.2000
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: forward, reward: -0.385819088893
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'left'), 'deadline': 3, 't': 27, 'action': 'forward', 'reward': -0.3858190888926052, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.385819088893, Q[state]-after: {'forward': -0.1929095444463026, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', None, 'left')
Agent drove forward instead of right. (rewarded -0.39)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Environment.step(): t = 28
Agent selected maxQ 0.3874 >= 0.2000
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: right, reward: 1.21039855055
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 28, 'action': 'right', 'reward': 1.210398550545098, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.7361902866459422, None: 1.6747368259573536, 'left': -9.967344384190643}, action: right, reward: 1.21039855055, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.6747368259573536, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent followed the waypoint right. (rewarded 1.21)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Environment.step(): t = 29
Agent selected maxQ 0.4954 >= 0.2000
Environment.act() [POST]: location: (3, 7), heading: (1, 0), action: None, reward: 0.563187546905
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 1, 't': 29, 'action': None, 'reward': 0.5631875469050935, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'left', None), Q[state]-before: {'forward': -19.654121751094024, 'right': -10.306940467082867, None: 0.0, 'left': 0.0}, action: None, reward: 0.563187546905, Q[state]-after: {'forward': -19.654121751094024, 'right': -10.306940467082867, None: 0.28159377345254677, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent properly idled at a red light. (rewarded 0.56)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 17
\-------------------------

Environment.reset(): Trial set up with start = (2, 3), destination = (4, 5), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.5051 >= 0.1500
Environment.act() [POST]: location: (2, 4), heading: (0, 1), action: right, reward: 0.756779237694
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 0.7567792376942148, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.9846215919864523, None: 0.0, 'left': 0.16631087142739243}, action: right, reward: 0.756779237694, Q[state]-after: {'forward': 0.0, 'right': 0.8707004148403336, None: 0.0, 'left': 0.16631087142739243}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent decided to explore 0.1087 < 0.1500
Environment.act() [POST]: location: (2, 5), heading: (0, 1), action: forward, reward: 0.080407092774
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 0.08040709277399516, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: forward, reward: 0.080407092774, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent drove forward instead of left. (rewarded 0.08)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.2067 >= 0.1500
Environment.act() [POST]: location: (1, 5), heading: (-1, 0), action: right, reward: 0.399421265106
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': 0.3994212651060166, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.260260184728557, 'right': 1.0974068162684505, None: 0.0, 'left': 0.0}, action: right, reward: 0.399421265106, Q[state]-after: {'forward': 0.260260184728557, 'right': 0.7484140406872335, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, None)
Agent drove right instead of left. (rewarded 0.40)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.4548 >= 0.1500
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: forward, reward: 1.5069823638
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'forward'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 1.5069823637986133, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.847079936279374, 'left': 0.0}, action: forward, reward: 1.5069823638, Q[state]-after: {'forward': 0.7534911818993066, 'right': 0.0, None: -2.847079936279374, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'forward', 'forward')
Agent drove forward instead of right. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.6107 >= 0.1500
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: 1.38057736053
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.3805773605253349, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'right'), Q[state]-before: {'forward': -5.005201576035521, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.38057736053, Q[state]-after: {'forward': -5.005201576035521, 'right': 0.6902886802626674, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'right')
Agent followed the waypoint right. (rewarded 1.38)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9323 >= 0.1500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 2.55067107835
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 2.5506710783468702, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, None), Q[state]-before: {'forward': -4.682235054855685, 'right': 0.0, None: 0.0, 'left': -5.44236463690834}, action: right, reward: 2.55067107835, Q[state]-after: {'forward': -4.682235054855685, 'right': 1.2753355391734351, None: 0.0, 'left': -5.44236463690834}
Agent previous state: ('right', 'red', 'right', None, None)
Agent followed the waypoint right. (rewarded 2.55)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.4945 >= 0.1500
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: right, reward: 0.956260179598
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 0.9562601795980037, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.8707004148403336, None: 0.0, 'left': 0.16631087142739243}, action: right, reward: 0.956260179598, Q[state]-after: {'forward': 0.0, 'right': 0.9134802972191687, None: 0.0, 'left': 0.16631087142739243}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.96)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.5267 >= 0.1500
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: -39.8563041427
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -39.85630414266365, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.8563041427, Q[state]-after: {'forward': -19.928152071331827, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.9709 >= 0.1500
Environment.act() [POST]: location: (8, 5), heading: (-1, 0), action: right, reward: 0.0759266658164
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 0.07592666581639962, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', None), Q[state]-before: {'forward': -7.426550813474842, 'right': 0.38591291334248334, None: 0.0, 'left': 0.0}, action: right, reward: 0.0759266658164, Q[state]-after: {'forward': -7.426550813474842, 'right': 0.23091978957944148, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', None)
Agent drove right instead of left. (rewarded 0.08)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.3030 >= 0.1500
Environment.act() [POST]: location: (8, 4), heading: (0, -1), action: right, reward: 2.29114886412
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'left'), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 2.291148864118827, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, 'left'), Q[state]-before: {'forward': -0.1929095444463026, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.29114886412, Q[state]-after: {'forward': -0.1929095444463026, 'right': 1.1455744320594134, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 2.29)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.1983 >= 0.1500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: right, reward: 1.7507876987
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 1.7507876987044289, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'right'), Q[state]-before: {'forward': -5.005201576035521, 'right': 0.6902886802626674, None: 0.0, 'left': 0.0}, action: right, reward: 1.7507876987, Q[state]-after: {'forward': -5.005201576035521, 'right': 1.220538189483548, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'right')
Agent followed the waypoint right. (rewarded 1.75)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.2447 >= 0.1500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: 1.47450911908
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'forward', 'left', None), 'deadline': 9, 't': 11, 'action': None, 'reward': 1.4745091190755453, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'left', None), Q[state]-before: {'forward': -19.654121751094024, 'right': -10.306940467082867, None: 0.28159377345254677, 'left': 0.0}, action: None, reward: 1.47450911908, Q[state]-after: {'forward': -19.654121751094024, 'right': -10.306940467082867, None: 0.8780514462640461, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'left', None)
Agent properly idled at a red light. (rewarded 1.47)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.8743 >= 0.1500
Environment.act() [POST]: location: (1, 4), heading: (1, 0), action: None, reward: 1.31309147066
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': None, 'reward': 1.3130914706601116, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.135353570386192, 'left': -9.115363129092575}, action: None, reward: 1.31309147066, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.7242225205231518, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.4295 >= 0.1500
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: left, reward: 1.45253720939
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 1.4525372093940123, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0459254650308796}, action: left, reward: 1.45253720939, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.249231337212446}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.45)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.6571 >= 0.1500
Environment.act() [POST]: location: (8, 3), heading: (-1, 0), action: left, reward: 0.147326644887
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': 0.14732664488737057, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.015486320137508613, 'right': 0.0, None: 0.0, 'left': 0.8426216787457506}, action: left, reward: 0.147326644887, Q[state]-after: {'forward': 0.015486320137508613, 'right': 0.0, None: 0.0, 'left': 0.49497416181656056}
Agent previous state: ('right', 'green', 'left', 'forward', None)
Agent drove left instead of right. (rewarded 0.15)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.8386 >= 0.1500
Environment.act() [POST]: location: (7, 3), heading: (-1, 0), action: forward, reward: -0.412193952425
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -0.4121939524250924, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.9621555874165295, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8874637905489268}, action: forward, reward: -0.412193952425, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8874637905489268}
Agent previous state: ('left', 'green', None, None, None)
Agent drove forward instead of left. (rewarded -0.41)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.8624 >= 0.1500
Environment.act() [POST]: location: (6, 3), heading: (-1, 0), action: forward, reward: 0.805943045956
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'left'), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 0.8059430459562344, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', 'left'), Q[state]-before: {'forward': 1.0068717735900659, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.805943045956, Q[state]-after: {'forward': 0.9064074097731502, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', 'left')
Agent followed the waypoint forward. (rewarded 0.81)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.8708 >= 0.1500
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: right, reward: 0.521881441522
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 0.5218814415221921, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.571217210367369, None: 0.0, 'left': 0.0}, action: right, reward: 0.521881441522, Q[state]-after: {'forward': 0.0, 'right': 0.5465493259447806, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.52)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.7314 >= 0.1500
Environment.act() [POST]: location: (6, 2), heading: (0, -1), action: None, reward: 0.939111691203
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.9391116912030655, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5139224393066544, 'left': -9.153640767014467}, action: None, reward: 0.939111691203, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.22651706525486, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.8706 >= 0.1500
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: left, reward: 0.903485095472
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 1, 't': 19, 'action': 'left', 'reward': 0.9034850954715814, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8874637905489268}, action: left, reward: 0.903485095472, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8954744430102541}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 0.90)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 18
\-------------------------

Environment.reset(): Trial set up with start = (6, 7), destination = (1, 5), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.7214 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 2.00280319172
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': 2.00280319172433, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', None), Q[state]-before: {'forward': -5.251348542493676, 'right': 0.3992640909760684, None: 0.0, 'left': -6.954336507814753}, action: right, reward: 2.00280319172, Q[state]-after: {'forward': -5.251348542493676, 'right': 1.2010336413501992, None: 0.0, 'left': -6.954336507814753}
Agent previous state: ('right', 'red', None, 'right', None)
Agent followed the waypoint right. (rewarded 2.00)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.2548 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: -10.538817753
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'right', 'forward'), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': -10.538817752968137, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.538817753, Q[state]-after: {'forward': -5.269408876484069, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'right', 'forward')
Agent attempted driving forward through a red light. (rewarded -10.54)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.5413 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: None, reward: 1.9154520047
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 23, 't': 2, 'action': None, 'reward': 1.915452004700269, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.9654619652537475, 'left': 0.0}, action: None, reward: 1.9154520047, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.9404569849770081, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 1.92)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9335 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: -10.2380434953
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'left', None, 'forward'), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': -10.238043495322948, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.2380434953, Q[state]-after: {'forward': -5.119021747661474, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', None, 'forward')
Agent attempted driving forward through a red light. (rewarded -10.24)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.2390 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: None, reward: 2.54450526258
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.544505262584555, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 0.7652419863299766, 'left': 0.0}, action: None, reward: 2.54450526258, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.5914 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: -40.0368512874
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': None}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', None, 'forward', 'forward'), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -40.036851287387755, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.591728276243682}, action: forward, reward: -40.0368512874, Q[state]-after: {'forward': -20.018425643693877, 'right': 0.0, None: 0.0, 'left': -19.591728276243682}
Agent previous state: ('right', 'red', None, 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.04)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.1769 >= 0.1000
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: 2.25815462546
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 19, 't': 6, 'action': 'right', 'reward': 2.258154625455278, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.300089730189393, None: -3.8737587374268805, 'left': 0.0}, action: right, reward: 2.25815462546, Q[state]-after: {'forward': 0.0, 'right': 1.7791221778223356, None: -3.8737587374268805, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.6865 >= 0.1000
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: right, reward: 1.52497766237
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 1.5249776623709237, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, None), Q[state]-before: {'forward': -5.23193719797432, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.52497766237, Q[state]-after: {'forward': -5.23193719797432, 'right': 0.7624888311854618, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, None)
Agent drove right instead of forward. (rewarded 1.52)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.3326 >= 0.1000
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: right, reward: 1.14703072893
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.1470307289308024, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.403445317057189, None: 1.3356536212394092, 'left': 0.0}, action: right, reward: 1.14703072893, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.2752380229939957, None: 1.3356536212394092, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent drove right instead of left. (rewarded 1.15)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.7014 >= 0.1000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 2.16178855358
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 16, 't': 9, 'action': 'right', 'reward': 2.161788553582139, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -7.369319515974606, 'right': 0.6935289350999396, None: 0.0, 'left': 0.0}, action: right, reward: 2.16178855358, Q[state]-after: {'forward': -7.369319515974606, 'right': 1.4276587443410391, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 2.16)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent decided to explore 0.0421 < 0.1000
Environment.act() [POST]: location: (5, 6), heading: (-1, 0), action: left, reward: -0.147739439476
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 15, 't': 10, 'action': 'left', 'reward': -0.14773943947570445, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.8854462406807438}, action: left, reward: -0.147739439476, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded -0.15)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.1162 >= 0.1000
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: right, reward: 2.48022075502
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', None, None), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 2.4802207550190083, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 1.0440590884333745, None: 0.0, 'left': 0.0}, action: right, reward: 2.48022075502, Q[state]-after: {'forward': 0.0, 'right': 1.7621399217261913, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', None, None)
Agent followed the waypoint right. (rewarded 2.48)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.1399 >= 0.1000
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 2.64693615319
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 13, 't': 12, 'action': None, 'reward': 2.6469361531871556, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.097661613398911, 'left': 0.0}, action: None, reward: 2.64693615319, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.65)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.7778 >= 0.1000
Environment.act() [POST]: location: (5, 5), heading: (0, -1), action: None, reward: 0.553500666644
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 12, 't': 13, 'action': None, 'reward': 0.5535006666443567, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.7172399300892507, 'left': -0.09759033182575594}, action: None, reward: 0.553500666644, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.6353702983668037, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.55)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.7922 >= 0.1000
Environment.act() [POST]: location: (6, 5), heading: (1, 0), action: right, reward: 2.48859340468
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 2.488593404679503, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.7791221778223356, None: -3.8737587374268805, 'left': 0.0}, action: right, reward: 2.48859340468, Q[state]-after: {'forward': 0.0, 'right': 2.1338577912509193, None: -3.8737587374268805, 'left': 0.0}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.4766 >= 0.1000
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: forward, reward: 2.14762413721
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', 'left'), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': 2.1476241372139233, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.14762413721, Q[state]-after: {'forward': 1.0738120686069617, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', 'forward', 'left')
Agent followed the waypoint forward. (rewarded 2.15)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.9819 >= 0.1000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: left, reward: 1.3432253922
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 9, 't': 16, 'action': 'left', 'reward': 1.343225392204936, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.5903175215277303}, action: left, reward: 1.3432253922, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.34)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent decided to explore 0.0208 < 0.1000
Environment.act() [POST]: location: (7, 4), heading: (0, -1), action: left, reward: -19.2368597912
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 8, 't': 17, 'action': 'left', 'reward': -19.236859791202022, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -10.479335763847477}, action: left, reward: -19.2368597912, Q[state]-after: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -14.85809777752475}
Agent previous state: ('right', 'green', None, None, 'right')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.24)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.1465 >= 0.1000
Environment.act() [POST]: location: (8, 4), heading: (1, 0), action: right, reward: 2.34435310916
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 7, 't': 18, 'action': 'right', 'reward': 2.344353109156558, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.5919780524679397, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 2.34435310916, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.968165580812249, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.34)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.2855 >= 0.1000
Environment.act() [POST]: location: (8, 3), heading: (0, -1), action: left, reward: 1.21897440289
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 6, 't': 19, 'action': 'left', 'reward': 1.218974402887448, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.249231337212446}, action: left, reward: 1.21897440289, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.234102870049947}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.22)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent selected maxQ 0.9380 >= 0.1000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: right, reward: 1.78411913904
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, 'left'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 1.7841191390447655, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, 'left'), Q[state]-before: {'forward': -0.1929095444463026, 'right': 1.1455744320594134, None: 0.0, 'left': 0.0}, action: right, reward: 1.78411913904, Q[state]-after: {'forward': -0.1929095444463026, 'right': 1.4648467855520895, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 1.78)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.2355 >= 0.1000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: None, reward: 1.38861765017
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 4, 't': 21, 'action': None, 'reward': 1.3886176501706697, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 2.1181698396490294, 'left': 0.0}, action: None, reward: 1.38861765017, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.7533937449098496, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 1.39)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent selected maxQ 0.9900 >= 0.1000
Environment.act() [POST]: location: (1, 3), heading: (1, 0), action: None, reward: 1.52153650035
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 3, 't': 22, 'action': None, 'reward': 1.5215365003479102, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.6747368259573536, 'left': -9.967344384190643}, action: None, reward: 1.52153650035, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.598136663152632, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.5895 >= 0.1000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: right, reward: 0.36063679487
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'right'), 'deadline': 2, 't': 23, 'action': 'right', 'reward': 0.360636794869754, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'right'), Q[state]-before: {'forward': -5.005201576035521, 'right': 1.220538189483548, None: 0.0, 'left': 0.0}, action: right, reward: 0.36063679487, Q[state]-after: {'forward': -5.005201576035521, 'right': 0.790587492176651, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'right')
Agent followed the waypoint right. (rewarded 0.36)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.6638 >= 0.1000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: forward, reward: -9.2535687268
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': -9.253568726799445, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -4.759832858400417}, action: forward, reward: -9.2535687268, Q[state]-after: {'forward': -4.6267843633997225, 'right': 0.0, None: 0.0, 'left': -4.759832858400417}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, 'right', None)
Agent attempted driving forward through a red light. (rewarded -9.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 19
\-------------------------

Environment.reset(): Trial set up with start = (6, 7), destination = (2, 2), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.7921 >= 0.0500
Environment.act() [POST]: location: (6, 2), heading: (0, 1), action: forward, reward: 0.378976423559
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'left', 'right'), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': 0.37897642355941596, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.378976423559, Q[state]-after: {'forward': 0.18948821177970798, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', 'left', 'right')
Agent drove forward instead of left. (rewarded 0.38)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.6211 >= 0.0500
Environment.act() [POST]: location: (6, 3), heading: (0, 1), action: forward, reward: 1.30023262329
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'left', None), 'deadline': 24, 't': 1, 'action': 'forward', 'reward': 1.3002326232913468, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'left', None), Q[state]-before: {'forward': 0.9481224725693573, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.30023262329, Q[state]-after: {'forward': 1.1241775479303522, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'left', None)
Agent drove forward instead of left. (rewarded 1.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.3364 >= 0.0500
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: left, reward: 1.77402883676
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'left', None), 'deadline': 23, 't': 2, 'action': 'left', 'reward': 1.7740288367558346, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9937237892479429}, action: left, reward: 1.77402883676, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 1.3838763130018887}
Agent previous state: ('left', 'green', 'forward', 'left', None)
Agent followed the waypoint left. (rewarded 1.77)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9342 >= 0.0500
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: forward, reward: 1.4471783144
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'right'), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': 1.447178314396907, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.4471783144, Q[state]-after: {'forward': 0.7235891571984535, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', None, 'right')
Agent followed the waypoint forward. (rewarded 1.45)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.2532 >= 0.0500
Environment.act() [POST]: location: (8, 3), heading: (1, 0), action: None, reward: 2.86718715542
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', None), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.8671871554197876, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', None), Q[state]-before: {'forward': -4.718442006745252, 'right': 0.6025619083949016, None: 1.2202756914838786, 'left': 0.0}, action: None, reward: 2.86718715542, Q[state]-after: {'forward': -4.718442006745252, 'right': 0.6025619083949016, None: 2.043731423451833, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', None)
Agent properly idled at a red light. (rewarded 2.87)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.1360 >= 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: 1.23702205783
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 1.2370220578267914, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.234102870049947}, action: left, reward: 1.23702205783, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.2355624639383693}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.24)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.4763 >= 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 2.8660888181
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': 2.8660888180953936, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.598136663152632, 'left': -9.967344384190643}, action: None, reward: 2.8660888181, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 2.2321127406240127, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.4105 >= 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 2.84351037052
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 18, 't': 7, 'action': None, 'reward': 2.84351037052033, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 1.9404569849770081, 'left': 0.0}, action: None, reward: 2.84351037052, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 2.3919836777486694, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.84)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.4202 >= 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 2.18686482466
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.1868648246553692, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 2.3919836777486694, 'left': 0.0}, action: None, reward: 2.18686482466, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 2.2894242512020195, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 2.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent decided to explore 0.0455 < 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: left, reward: -20.5008745762
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 3, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 16, 't': 9, 'action': 'left', 'reward': -20.50087457622801, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 2.1338577912509193, None: -3.8737587374268805, 'left': 0.0}, action: left, reward: -20.5008745762, Q[state]-after: {'forward': 0.0, 'right': 2.1338577912509193, None: -3.8737587374268805, 'left': -10.250437288114005}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.1946 >= 0.0500
Environment.act() [POST]: location: (8, 2), heading: (0, -1), action: None, reward: 0.603351621067
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 15, 't': 10, 'action': None, 'reward': 0.6033516210666897, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.6353702983668037, 'left': -0.09759033182575594}, action: None, reward: 0.603351621067, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.6193609597167467, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.60)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.3591 >= 0.0500
Environment.act() [POST]: location: (7, 2), heading: (-1, 0), action: left, reward: 1.76886575695
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 1.7688657569501167, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9548154609291288}, action: left, reward: 1.76886575695, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 1.3618406089396227}
Agent previous state: ('right', 'green', None, 'forward', None)
Agent drove left instead of right. (rewarded 1.77)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.8485 >= 0.0500
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: forward, reward: -0.125005439828
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -0.12500543982827006, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.2077950865359247, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.125005439828, Q[state]-after: {'forward': 0.041394823353827326, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent drove forward instead of right. (rewarded -0.13)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.6881 >= 0.0500
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: None, reward: 0.843505540228
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 12, 't': 13, 'action': None, 'reward': 0.8435055402280764, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, None), Q[state]-before: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.0, 'left': 0.0}, action: None, reward: 0.843505540228, Q[state]-after: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.4217527701140382, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 0.84)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.2185 >= 0.0500
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: None, reward: 0.744875803262
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 14, 'action': None, 'reward': 0.744875803262131, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 2.2321127406240127, 'left': -9.967344384190643}, action: None, reward: 0.744875803262, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.4884942719430718, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.74)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.4013 >= 0.0500
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: right, reward: 1.63200437789
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 1.6320043778885784, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.968165580812249, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.63200437789, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.8000849793504137, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.63)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.9952 >= 0.0500
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: forward, reward: -39.0996840496
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'left'), 'deadline': 9, 't': 16, 'action': 'forward', 'reward': -39.099684049557005, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.0996840496, Q[state]-after: {'forward': -19.549842024778503, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.10)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.9051 >= 0.0500
Environment.act() [POST]: location: (6, 7), heading: (0, -1), action: None, reward: 0.417921016806
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 8, 't': 17, 'action': None, 'reward': 0.41792101680560045, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.6193609597167467, 'left': -0.09759033182575594}, action: None, reward: 0.417921016806, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.42)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.9994 >= 0.0500
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: forward, reward: 0.492901614162
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', 'forward', 'left'), 'deadline': 7, 't': 18, 'action': 'forward', 'reward': 0.49290161416241607, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.492901614162, Q[state]-after: {'forward': 0.24645080708120803, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'left', 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.49)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.8732 >= 0.0500
Environment.act() [POST]: location: (6, 5), heading: (0, -1), action: forward, reward: 0.471710765563
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', 'right'), 'deadline': 6, 't': 19, 'action': 'forward', 'reward': 0.47171076556290303, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.471710765563, Q[state]-after: {'forward': 0.23585538278145152, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'left', 'right')
Agent drove forward instead of right. (rewarded 0.47)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent selected maxQ 0.6998 >= 0.0500
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: right, reward: 2.02646375767
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 2.026463757670457, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.8000849793504137, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 2.02646375767, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.9132743685104354, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.03)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.9865 >= 0.0500
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: None, reward: 2.0422723786
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 2.042272378597196, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.7242225205231518, 'left': -9.115363129092575}, action: None, reward: 2.0422723786, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8832474495601739, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent selected maxQ 0.6568 >= 0.0500
Environment.act() [POST]: location: (7, 5), heading: (1, 0), action: None, reward: 2.23966315619
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 3, 't': 22, 'action': None, 'reward': 2.23966315618552, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 1.8832474495601739, 'left': -9.115363129092575}, action: None, reward: 2.23966315619, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.061455302872847, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.5063 >= 0.0500
Environment.act() [POST]: location: (8, 5), heading: (1, 0), action: forward, reward: 1.15527229555
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 2, 't': 23, 'action': 'forward', 'reward': 1.1552722955469816, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'forward'), Q[state]-before: {'forward': 1.5891261872621816, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.15527229555, Q[state]-after: {'forward': 1.3721992414045816, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 1.16)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.1646 >= 0.0500
Environment.act() [POST]: location: (1, 5), heading: (1, 0), action: forward, reward: 0.925556135066
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 1, 't': 24, 'action': 'forward', 'reward': 0.9255561350659849, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, None), Q[state]-before: {'forward': 1.2802993385908468, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}, action: forward, reward: 0.925556135066, Q[state]-after: {'forward': 1.1029277368284158, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 0.93)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 20
\-------------------------

Environment.reset(): Trial set up with start = (8, 6), destination = (5, 5), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.3175 >= -0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: forward, reward: 1.62076605005
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 1.620766050049299, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.62076605005, Q[state]-after: {'forward': 0.8103830250246495, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', 'forward')
Agent drove forward instead of right. (rewarded 1.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.0035 >= -0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: None, reward: 2.69339829603
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.693398296027654, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.4884942719430718, 'left': -9.967344384190643}, action: None, reward: 2.69339829603, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 2.090946283985363, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.3440 >= -0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, 1), action: None, reward: 1.70797937483
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': None, 'reward': 1.7079793748264276, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 2.090946283985363, 'left': -9.967344384190643}, action: None, reward: 1.70797937483, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.6907 >= -0.0000
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: right, reward: 2.05566259353
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 2.0556625935275163, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -7.369319515974606, 'right': 1.4276587443410391, None: 0.0, 'left': 0.0}, action: right, reward: 2.05566259353, Q[state]-after: {'forward': -7.369319515974606, 'right': 1.7416606689342777, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.6693 >= -0.0000
Environment.act() [POST]: location: (7, 7), heading: (-1, 0), action: None, reward: 2.43035977914
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.4303597791445255, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.061455302872847, 'left': -9.115363129092575}, action: None, reward: 2.43035977914, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.43)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.1029 >= -0.0000
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: left, reward: 0.957477999165
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 0.9574779991653375, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.2355624639383693}, action: left, reward: 0.957477999165, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.4490 >= -0.0000
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: right, reward: 2.52482278199
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', None, None), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 2.524822781987079, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 1.2489986181945403, None: 0.0, 'left': 0.7045550375166743}, action: right, reward: 2.52482278199, Q[state]-after: {'forward': 0.0, 'right': 1.8869107000908096, None: 0.0, 'left': 0.7045550375166743}
Agent previous state: ('right', 'green', 'right', None, None)
Agent followed the waypoint right. (rewarded 2.52)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.7945 >= -0.0000
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: forward, reward: 2.33427676711
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, 'forward'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 2.334276767114894, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': -0.44224376702750157, None: 0.0, 'left': 0.0}, action: forward, reward: 2.33427676711, Q[state]-after: {'forward': 1.167138383557447, 'right': -0.44224376702750157, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', None, 'forward')
Agent followed the waypoint forward. (rewarded 2.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.8242 >= -0.0000
Environment.act() [POST]: location: (4, 2), heading: (-1, 0), action: forward, reward: 0.821137260848
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'forward', None), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 0.8211372608476756, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.041394823353827326, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.821137260848, Q[state]-after: {'forward': 0.4312660421007515, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'forward', None)
Agent drove forward instead of right. (rewarded 0.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.5019 >= -0.0000
Environment.act() [POST]: location: (4, 7), heading: (0, -1), action: right, reward: 1.30063054733
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.300630547325326, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.8210703228396621, None: 0.0, 'left': 0.0}, action: right, reward: 1.30063054733, Q[state]-after: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent followed the waypoint right. (rewarded 1.30)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.1133 >= -0.0000
Environment.act() [POST]: location: (5, 7), heading: (1, 0), action: right, reward: 0.990628868568
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 10, 't': 10, 'action': 'right', 'reward': 0.9906288685679969, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 2.1338577912509193, None: -3.8737587374268805, 'left': -10.250437288114005}, action: right, reward: 0.990628868568, Q[state]-after: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 0.99)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.0036 >= -0.0000
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: forward, reward: 1.61232950552
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', None), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': 1.612329505520966, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', None), Q[state]-before: {'forward': 0.9130960058725955, 'right': 0.0, None: -2.1845675904383586, 'left': 0.0}, action: forward, reward: 1.61232950552, Q[state]-after: {'forward': 1.2627127556967808, 'right': 0.0, None: -2.1845675904383586, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', None)
Agent drove forward instead of left. (rewarded 1.61)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.4329 >= -0.0000
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: None, reward: 1.93203418774
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': None, 'reward': 1.9320341877371296, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.22651706525486, 'left': -9.153640767014467}, action: None, reward: 1.93203418774, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.3529 >= -0.0000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: left, reward: 2.15574871858
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 2.155748718575828, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 0.8954744430102541}, action: left, reward: 2.15574871858, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.16)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.5653 >= -0.0000
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: 1.43021694667
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 1.4302169466660302, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.658791584185761, 'right': 0.8976092716361164, None: 0.0, 'left': 0.0}, action: right, reward: 1.43021694667, Q[state]-after: {'forward': -4.658791584185761, 'right': 1.1639131091510733, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.43)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.9962 >= -0.0000
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: forward, reward: -40.8564041631
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'forward'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -40.856404163108536, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.8564041631, Q[state]-after: {'forward': -20.428202081554268, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'left', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.86)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.0557 >= -0.0000
Environment.act() [POST]: location: (7, 6), heading: (1, 0), action: right, reward: -20.2662243156
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'left', 'forward'), 'deadline': 4, 't': 16, 'action': 'right', 'reward': -20.266224315614423, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'left', 'forward'), Q[state]-before: {'forward': -20.428202081554268, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -20.2662243156, Q[state]-after: {'forward': -20.428202081554268, 'right': -10.133112157807211, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'left', 'forward')
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.27)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.1242 >= -0.0000
Environment.act() [POST]: location: (7, 7), heading: (0, 1), action: right, reward: 1.16295646543
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 3, 't': 17, 'action': 'right', 'reward': 1.1629564654274556, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.8035766461757469, None: 0.0, 'left': -10.408944164083675}, action: right, reward: 1.16295646543, Q[state]-after: {'forward': 0.0, 'right': 0.9832665558016012, None: 0.0, 'left': -10.408944164083675}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.16)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.0927 >= -0.0000
Environment.act() [POST]: location: (6, 7), heading: (-1, 0), action: right, reward: 1.66658262899
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 1.6665826289903085, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.9132743685104354, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.66658262899, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.67)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.6243 >= -0.0000
Environment.act() [POST]: location: (6, 6), heading: (0, -1), action: right, reward: 0.0991815790728
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 1, 't': 19, 'action': 'right', 'reward': 0.09918157907278669, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', None), Q[state]-before: {'forward': -4.6267843633997225, 'right': 0.0, None: 0.0, 'left': -4.759832858400417}, action: right, reward: 0.0991815790728, Q[state]-after: {'forward': -4.6267843633997225, 'right': 0.04959078953639334, None: 0.0, 'left': -4.759832858400417}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'red', None, 'right', None)
Agent drove right instead of forward. (rewarded 0.10)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 1
\-------------------------

Environment.reset(): Trial set up with start = (1, 5), destination = (4, 6), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.4584 >= 0.0000
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: 1.51080583296
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 1.5108058329568748, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': -7.669334934958838, 'right': 0.36188645308689993, None: 0.0, 'left': 0.0}, action: right, reward: 1.51080583296, Q[state]-after: {'forward': -7.669334934958838, 'right': 0.36188645308689993, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 1.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.5634 >= 0.0000
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: None, reward: 2.78979245152
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.7897924515216563, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.78979245152, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.79)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6279 >= 0.0000
Environment.act() [POST]: location: (1, 6), heading: (0, 1), action: right, reward: -19.5567932736
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', None, None), 'deadline': 18, 't': 2, 'action': 'right', 'reward': -19.556793273567195, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, None), Q[state]-before: {'forward': -20.40006672331514, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -19.5567932736, Q[state]-after: {'forward': -20.40006672331514, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.0026 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (1, 0), action: left, reward: 1.40018954489
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 1.400189544885717, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 1.40018954489, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.7858 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (0, 1), action: right, reward: 1.15962018374
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.1596201837447881, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 1.15962018374, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.7822 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (-1, 0), action: right, reward: 1.80306010279
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'forward'), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 1.803060102792833, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.9832665558016012, None: 0.0, 'left': -10.408944164083675}, action: right, reward: 1.80306010279, Q[state]-after: {'forward': 0.0, 'right': 0.9832665558016012, None: 0.0, 'left': -10.408944164083675}
Agent previous state: ('left', 'green', None, None, 'forward')
Agent drove right instead of left. (rewarded 1.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.3648 >= 0.0000
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: left, reward: 1.79749234779
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 1.7974923477941482, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}, action: left, reward: 1.79749234779, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.1739 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 0.493135577573
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 0.4931355775733426, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}, action: right, reward: 0.493135577573, Q[state]-after: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent drove right instead of left. (rewarded 0.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.6920 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 1.64084155165
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.6408415516549986, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.64084155165, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.1940 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: None, reward: 1.60394220752
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.6039422075185006, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}, action: None, reward: 1.60394220752, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.5616 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: forward, reward: -39.7091668186
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'right', 'forward'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -39.70916681857577, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.7091668186, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'right', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.71)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.4528 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: right, reward: 1.81920452628
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 1.8192045262756273, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}, action: right, reward: 1.81920452628, Q[state]-after: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 1.82)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.6898 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 2.11374873538
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 8, 't': 12, 'action': None, 'reward': 2.1137487353818294, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 2.11374873538, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.6388 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: 1.70700473592
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'forward'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 1.707004735916102, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'forward'), Q[state]-before: {'forward': 1.3721992414045816, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.70700473592, Q[state]-after: {'forward': 1.3721992414045816, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'forward')
Agent followed the waypoint forward. (rewarded 1.71)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.8929 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: None, reward: 1.72334496849
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 6, 't': 14, 'action': None, 'reward': 1.7233449684933384, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 1.72334496849, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.4074 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, -1), action: left, reward: -0.00590450624939
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': 'left', 'reward': -0.005904506249387653, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}, action: left, reward: -0.00590450624939, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded -0.01)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.9823 >= 0.0000
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: right, reward: 2.23718309684
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 2.2371830968427378, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.089686061455693, None: 0.0, 'left': 0.0}, action: right, reward: 2.23718309684, Q[state]-after: {'forward': 0.0, 'right': 1.089686061455693, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 2.24)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.7418 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (0, 1), action: right, reward: -0.0895479033134
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': -0.08954790331339468, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.5465493259447806, None: 0.0, 'left': 0.0}, action: right, reward: -0.0895479033134, Q[state]-after: {'forward': 0.0, 'right': 0.5465493259447806, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded -0.09)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.4054 >= 0.0000
Environment.act() [POST]: location: (4, 7), heading: (1, 0), action: left, reward: 1.18531409865
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 2, 't': 18, 'action': 'left', 'reward': 1.1853140986483324, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 1.18531409865, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.19)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.0531 >= 0.0000
Environment.act() [POST]: location: (4, 7), heading: (1, 0), action: None, reward: 1.70250376589
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 1, 't': 19, 'action': None, 'reward': 1.7025037658940705, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.70250376589, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 2
\-------------------------

Environment.reset(): Trial set up with start = (1, 7), destination = (5, 6), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.4885 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: -39.4350279666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'right', None), 'deadline': 25, 't': 0, 'action': 'forward', 'reward': -39.43502796662436, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.4350279666, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.44)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.2646 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: None, reward: 1.28359832951
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.2835983295095523, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.28359832951, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.6076 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: -39.3299743921
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': -39.3299743921302, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.3299743921, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.9341 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: 0.554086872389
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', None, 'forward'), 'deadline': 22, 't': 3, 'action': 'forward', 'reward': 0.5540868723885388, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.554086872389, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', None, 'forward')
Agent drove forward instead of left. (rewarded 0.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.9315 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: None, reward: 2.88397045833
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 21, 't': 4, 'action': None, 'reward': 2.8839704583334242, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 2.88397045833, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9145 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, -1), action: left, reward: 1.45253888289
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 1.4525388828945727, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}, action: left, reward: 1.45253888289, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.45)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.3744 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, -1), action: None, reward: 1.42099176836
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': 1.4209917683595443, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, None), Q[state]-before: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.4217527701140382, 'left': 0.0}, action: None, reward: 1.42099176836, Q[state]-after: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.4217527701140382, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.42)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.5380 >= 0.0000
Environment.act() [POST]: location: (3, 6), heading: (1, 0), action: right, reward: 2.39312024402
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 18, 't': 7, 'action': 'right', 'reward': 2.3931202440228203, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -7.369319515974606, 'right': 1.7416606689342777, None: 0.0, 'left': 0.0}, action: right, reward: 2.39312024402, Q[state]-after: {'forward': -7.369319515974606, 'right': 1.7416606689342777, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 2.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.0741 >= 0.0000
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: left, reward: 1.21869550243
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 17, 't': 8, 'action': 'left', 'reward': 1.2186955024296027, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}, action: left, reward: 1.21869550243, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.3623 >= 0.0000
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: 1.60699943475
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 16, 't': 9, 'action': None, 'reward': 1.6069994347540417, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}, action: None, reward: 1.60699943475, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.0796 >= 0.0000
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: None, reward: 1.98586400044
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 15, 't': 10, 'action': None, 'reward': 1.9858640004400039, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}, action: None, reward: 1.98586400044, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 1.99)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.5510 >= 0.0000
Environment.act() [POST]: location: (4, 5), heading: (1, 0), action: right, reward: 1.46504397374
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'right', 'forward'), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 1.465043973740664, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'right', 'forward'), Q[state]-before: {'forward': -5.269408876484069, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.46504397374, Q[state]-after: {'forward': -5.269408876484069, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'right', 'forward')
Agent followed the waypoint right. (rewarded 1.47)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.7568 >= 0.0000
Environment.act() [POST]: location: (4, 4), heading: (0, -1), action: left, reward: 0.841569008746
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 13, 't': 12, 'action': 'left', 'reward': 0.8415690087459424, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}, action: left, reward: 0.841569008746, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 0.84)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.0395 >= 0.0000
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: right, reward: 1.87273300971
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', 'forward'), 'deadline': 12, 't': 13, 'action': 'right', 'reward': 1.872733009706632, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': -4.887026397323535, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 1.87273300971, Q[state]-after: {'forward': -4.887026397323535, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', 'forward')
Agent followed the waypoint right. (rewarded 1.87)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.0548 >= 0.0000
Environment.act() [POST]: location: (5, 5), heading: (0, 1), action: right, reward: 2.17051737179
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 2.170517371794654, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, None), Q[state]-before: {'forward': -4.682235054855685, 'right': 1.2753355391734351, None: 0.0, 'left': -5.44236463690834}, action: right, reward: 2.17051737179, Q[state]-after: {'forward': -4.682235054855685, 'right': 1.2753355391734351, None: 0.0, 'left': -5.44236463690834}
Agent previous state: ('right', 'red', 'right', None, None)
Agent followed the waypoint right. (rewarded 2.17)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.0320 >= 0.0000
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (5, 6), heading: (0, 1), action: forward, reward: 1.23097887508
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'right', None), 'deadline': 10, 't': 15, 'action': 'forward', 'reward': 1.2309788750778146, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.23097887508, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', 'right', None)
Agent followed the waypoint forward. (rewarded 1.23)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 3
\-------------------------

Environment.reset(): Trial set up with start = (3, 5), destination = (7, 5), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.3773 >= 0.0000
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: right, reward: 0.578861873777
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'left', 'left'), 'deadline': 20, 't': 0, 'action': 'right', 'reward': 0.5788618737770659, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'left', 'left'), Q[state]-before: {'forward': -5.448084210718792, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.578861873777, Q[state]-after: {'forward': -5.448084210718792, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'left', 'left')
Agent drove right instead of forward. (rewarded 0.58)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.1901 >= 0.0000
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: forward, reward: -9.55887425386
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', 'left', None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -9.558874253860012, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.55887425386, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'right', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.9736 >= 0.0000
Environment.act() [POST]: location: (3, 4), heading: (0, -1), action: None, reward: 1.71170092896
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': None, 'reward': 1.7117009289648761, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.71170092896, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.2364 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: left, reward: 2.20824421935
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 17, 't': 3, 'action': 'left', 'reward': 2.208244219353549, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: left, reward: 2.20824421935, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.5116 >= 0.0000
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: forward, reward: 1.05984830919
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', 'forward', 'left'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 1.0598483091911886, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.05984830919, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', 'forward', 'left')
Agent followed the waypoint forward. (rewarded 1.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.8292 >= 0.0000
Environment.act() [POST]: location: (8, 4), heading: (-1, 0), action: forward, reward: 1.70363555264
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 1.7036355526380895, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'left'), Q[state]-before: {'forward': 1.0639968326484772, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}, action: forward, reward: 1.70363555264, Q[state]-after: {'forward': 1.0639968326484772, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 1.70)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.5836 >= 0.0000
Environment.act() [POST]: location: (8, 5), heading: (0, 1), action: left, reward: 0.397305465836
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 0.3973054658362929, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}, action: left, reward: 0.397305465836, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 0.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.6243 >= 0.0000
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (7, 5), heading: (-1, 0), action: right, reward: 1.48280049447
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.4828004944709625, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.48280049447, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.48)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 4
\-------------------------

Environment.reset(): Trial set up with start = (2, 6), destination = (8, 2), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.1587 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: right, reward: -19.6105507869
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('forward', 'red', 'forward', 'right', None), 'deadline': 20, 't': 0, 'action': 'right', 'reward': -19.61055078689466, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', 'right', None), Q[state]-before: {'forward': -19.806973804886184, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -19.6105507869, Q[state]-after: {'forward': -19.806973804886184, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', 'right', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.7289 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: -40.106569003
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -40.106569003018976, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.106569003, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.8855 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: -40.9932856119
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', None), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -40.99328561187263, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.9932856119, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.99)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.5769 >= 0.0000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: right, reward: 0.38842652065
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 0.38842652065030336, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.5465493259447806, None: 0.0, 'left': 0.0}, action: right, reward: 0.38842652065, Q[state]-after: {'forward': 0.0, 'right': 0.5465493259447806, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, None)
Agent drove right instead of forward. (rewarded 0.39)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.8296 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: 0.892955215304
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'forward', 'forward', None), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': 0.8929552153035131, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.892955215304, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'forward', 'forward', None)
Agent drove forward instead of left. (rewarded 0.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.1748 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -40.6510401026
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'right'), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': -40.65104010258187, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.6510401026, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, 'right')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.65)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.2535 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: 1.96409388075
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 14, 't': 6, 'action': None, 'reward': 1.9640938807508974, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.96409388075, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.96)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.5073 >= 0.0000
Environment.act() [POST]: location: (3, 4), heading: (1, 0), action: right, reward: 1.34504776445
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 1.3450477644458836, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', None), Q[state]-before: {'forward': -7.426550813474842, 'right': 0.23091978957944148, None: 0.0, 'left': 0.0}, action: right, reward: 1.34504776445, Q[state]-after: {'forward': -7.426550813474842, 'right': 0.23091978957944148, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', None)
Agent drove right instead of left. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.0388 >= 0.0000
Environment.act() [POST]: location: (4, 4), heading: (1, 0), action: forward, reward: 1.32763658069
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'right'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 1.3276365806913222, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.32763658069, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', 'right')
Agent drove forward instead of left. (rewarded 1.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.7795 >= 0.0000
Environment.act() [POST]: location: (4, 5), heading: (0, 1), action: right, reward: 1.69849726288
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.6984972628829942, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}, action: right, reward: 1.69849726288, Q[state]-after: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent drove right instead of left. (rewarded 1.70)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.4012 >= 0.0000
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: forward, reward: 0.74271015056
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'forward', 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': 0.7427101505598205, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'forward', 'left'), Q[state]-before: {'forward': 0.3308570354325046, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.74271015056, Q[state]-after: {'forward': 0.3308570354325046, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'forward', 'left')
Agent drove forward instead of right. (rewarded 0.74)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.1559 >= 0.0000
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 2.25253911679
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 9, 't': 11, 'action': 'right', 'reward': 2.2525391167915774, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}, action: right, reward: 2.25253911679, Q[state]-after: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.25)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.3203 >= 0.0000
Environment.act() [POST]: location: (3, 5), heading: (0, -1), action: right, reward: -0.319617111409
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'forward', None), 'deadline': 8, 't': 12, 'action': 'right', 'reward': -0.3196171114087003, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.46113250391520455, None: 0.0, 'left': 0.0}, action: right, reward: -0.319617111409, Q[state]-after: {'forward': 0.0, 'right': 0.46113250391520455, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', 'forward', None)
Agent drove right instead of forward. (rewarded -0.32)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.8947 >= 0.0000
Environment.act() [POST]: location: (2, 5), heading: (-1, 0), action: left, reward: 2.41379516915
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 7, 't': 13, 'action': 'left', 'reward': 2.413795169154523, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 2.41379516915, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.41)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.5971 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: left, reward: 1.32553972765
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, None), 'deadline': 6, 't': 14, 'action': 'left', 'reward': 1.3255397276530636, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, None), Q[state]-before: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}, action: left, reward: 1.32553972765, Q[state]-after: {'forward': 0.6352413946333738, 'right': 0.4262628356292294, None: -4.547128095116976, 'left': 1.0965202315518534}
Agent previous state: ('forward', 'green', None, None, None)
Agent drove left instead of forward. (rewarded 1.33)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.6992 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 1.17762669427
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 1.1776266942664009, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, None), Q[state]-before: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.4217527701140382, 'left': 0.0}, action: None, reward: 1.17762669427, Q[state]-after: {'forward': -20.457186583012287, 'right': -10.168363476469551, None: 0.4217527701140382, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, None)
Agent properly idled at a red light. (rewarded 1.18)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.4217 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 1.07040111585
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 4, 't': 16, 'action': None, 'reward': 1.0704011158534075, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}, action: None, reward: 1.07040111585, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.07)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.4348 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (0, 1), action: None, reward: 0.524776712987
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 3, 't': 17, 'action': None, 'reward': 0.5247767129870965, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 0.524776712987, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.52)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.2862 >= 0.0000
Environment.act() [POST]: location: (1, 6), heading: (-1, 0), action: right, reward: 1.35187454947
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 1.3518745494724487, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.35187454947, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.35)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.1030 >= 0.0000
Environment.act() [POST]: location: (8, 6), heading: (-1, 0), action: forward, reward: 0.123664839295
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, None, 'left'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.12366483929526928, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, None, 'left'), Q[state]-before: {'forward': 1.0639968326484772, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}, action: forward, reward: 0.123664839295, Q[state]-after: {'forward': 1.0639968326484772, 'right': 0.0, None: -0.06877630307095378, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('forward', 'green', None, None, 'left')
Agent followed the waypoint forward. (rewarded 0.12)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 5
\-------------------------

Environment.reset(): Trial set up with start = (7, 4), destination = (2, 5), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.9842 >= 0.0000
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.95483556236
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 20, 't': 0, 'action': None, 'reward': 2.954835562364998, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.95483556236, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.1748 >= 0.0000
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.91893057187
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 1, 'action': None, 'reward': 2.918930571866019, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.91893057187, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.3989 >= 0.0000
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: forward, reward: -10.1712677369
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'right'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.171267736853189, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.1712677369, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -10.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.2415 >= 0.0000
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.40242375824
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 3, 'action': None, 'reward': 2.4024237582409205, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.40242375824, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.40)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.0399 >= 0.0000
Environment.act() [POST]: location: (7, 4), heading: (-1, 0), action: None, reward: 2.35418224049
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 16, 't': 4, 'action': None, 'reward': 2.3541822404947794, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.35418224049, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.35)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.1221 >= 0.0000
Environment.act() [POST]: location: (6, 4), heading: (-1, 0), action: forward, reward: 0.0865808742522
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 15, 't': 5, 'action': 'forward', 'reward': 0.08658087425222138, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, None), Q[state]-before: {'forward': 0.7563768079585016, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.0865808742522, Q[state]-after: {'forward': 0.7563768079585016, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 0.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.0485 >= 0.0000
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: right, reward: 1.53577475666
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'left', 'left'), 'deadline': 14, 't': 6, 'action': 'right', 'reward': 1.5357747566580684, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.7201746599754382, None: 0.0, 'left': 0.0}, action: right, reward: 1.53577475666, Q[state]-after: {'forward': 0.0, 'right': 0.7201746599754382, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'left', 'left')
Agent drove right instead of left. (rewarded 1.54)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.3384 >= 0.0000
Environment.act() [POST]: location: (7, 3), heading: (1, 0), action: right, reward: 2.4945326046
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', 'forward', None), 'deadline': 13, 't': 7, 'action': 'right', 'reward': 2.494532604604124, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', 'forward', None), Q[state]-before: {'forward': -30.158393881183557, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 2.4945326046, Q[state]-after: {'forward': -30.158393881183557, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', 'forward', None)
Agent followed the waypoint right. (rewarded 2.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.3865 >= 0.0000
Environment.act() [POST]: location: (7, 2), heading: (0, -1), action: left, reward: 0.92830640746
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', 'left', 'left'), 'deadline': 12, 't': 8, 'action': 'left', 'reward': 0.9283064074604576, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', 'left', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.15715644790542738}, action: left, reward: 0.92830640746, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.15715644790542738}
Agent previous state: ('forward', 'green', 'left', 'left', 'left')
Agent drove left instead of forward. (rewarded 0.93)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.5903 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: right, reward: 1.79675355853
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 11, 't': 9, 'action': 'right', 'reward': 1.796753558527006, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}, action: right, reward: 1.79675355853, Q[state]-after: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent followed the waypoint right. (rewarded 1.80)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.9787 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: forward, reward: -10.3927880794
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', None, 'right', 'forward'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -10.392788079387262, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.3927880794, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'right', 'forward')
Agent attempted driving forward through a red light. (rewarded -10.39)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.4544 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: None, reward: 1.55684473505
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'forward'), 'deadline': 9, 't': 11, 'action': None, 'reward': 1.5568447350534695, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.441224671771394, 'left': 0.0}, action: None, reward: 1.55684473505, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.441224671771394, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, 'forward')
Agent properly idled at a red light. (rewarded 1.56)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.1647 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (1, 0), action: None, reward: 1.30381864848
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'forward'), 'deadline': 8, 't': 12, 'action': None, 'reward': 1.3038186484831709, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.21055369500205623, 'left': 0.0}, action: None, reward: 1.30381864848, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.21055369500205623, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, 'forward')
Agent properly idled at a red light. (rewarded 1.30)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.0673 >= 0.0000
Environment.act() [POST]: location: (1, 2), heading: (1, 0), action: forward, reward: 1.45454828421
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'forward'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 1.4545482842146387, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.45454828421, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', 'forward')
Agent followed the waypoint forward. (rewarded 1.45)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.2291 >= 0.0000
Environment.act() [POST]: location: (1, 3), heading: (0, 1), action: right, reward: 0.00744274576565
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'right', None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 0.0074427457656490725, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'right', None), Q[state]-before: {'forward': -4.6267843633997225, 'right': 0.04959078953639334, None: 0.0, 'left': -4.759832858400417}, action: right, reward: 0.00744274576565, Q[state]-after: {'forward': -4.6267843633997225, 'right': 0.04959078953639334, None: 0.0, 'left': -4.759832858400417}
Agent previous state: ('forward', 'red', None, 'right', None)
Agent drove right instead of forward. (rewarded 0.01)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.0357 >= 0.0000
Environment.act() [POST]: location: (1, 4), heading: (0, 1), action: forward, reward: -0.295788176141
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, None), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -0.2957881761408572, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.295788176141, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', None, None)
Agent drove forward instead of left. (rewarded -0.30)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.4201 >= 0.0000
Environment.act() [POST]: location: (1, 5), heading: (0, 1), action: forward, reward: 0.604719330385
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, 'left'), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': 0.6047193303845249, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.604719330385, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, 'left')
Agent drove forward instead of left. (rewarded 0.60)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.3356 >= 0.0000
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (2, 5), heading: (1, 0), action: left, reward: 1.47765144371
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 3, 't': 17, 'action': 'left', 'reward': 1.4776514437099306, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: left, reward: 1.47765144371, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.48)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 6
\-------------------------

Environment.reset(): Trial set up with start = (4, 6), destination = (1, 2), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.5179 >= 0.0000
Environment.act() [POST]: location: (4, 6), heading: (0, 1), action: None, reward: 0.957625059014
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 25, 't': 0, 'action': None, 'reward': 0.9576250590135751, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 0.957625059014, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.96)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.9602 >= 0.0000
Environment.act() [POST]: location: (3, 6), heading: (-1, 0), action: right, reward: 1.87119400173
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 24, 't': 1, 'action': 'right', 'reward': 1.8711940017341409, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -14.85809777752475}, action: right, reward: 1.87119400173, Q[state]-after: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -14.85809777752475}
Agent previous state: ('right', 'green', None, None, 'right')
Agent followed the waypoint right. (rewarded 1.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.7284 >= 0.0000
Environment.act() [POST]: location: (2, 6), heading: (-1, 0), action: forward, reward: 2.93302811177
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, 'forward'), 'deadline': 23, 't': 2, 'action': 'forward', 'reward': 2.933028111770886, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, 'forward'), Q[state]-before: {'forward': 1.2675104629211704, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.93302811177, Q[state]-after: {'forward': 1.2675104629211704, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, 'forward')
Agent followed the waypoint forward. (rewarded 2.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.4846 >= 0.0000
Environment.act() [POST]: location: (2, 5), heading: (0, -1), action: right, reward: 0.477276079686
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 22, 't': 3, 'action': 'right', 'reward': 0.47727607968554886, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 0.477276079686, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.48)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.8459 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: 0.329913195823
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', None, None), 'deadline': 21, 't': 4, 'action': 'forward', 'reward': 0.3299131958227962, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', None, None), Q[state]-before: {'forward': 0.7563768079585016, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.329913195823, Q[state]-after: {'forward': 0.7563768079585016, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', None, None)
Agent drove forward instead of left. (rewarded 0.33)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.4709 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -39.7665360018
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', 'right', None), 'deadline': 20, 't': 5, 'action': 'forward', 'reward': -39.76653600183999, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'right', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.7665360018, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'right', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.77)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.2008 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: 1.4709934865
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 19, 't': 6, 'action': None, 'reward': 1.4709934864987098, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.4709934865, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.8398 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: forward, reward: -9.28560350118
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, None, 'right'), 'deadline': 18, 't': 7, 'action': 'forward', 'reward': -9.285603501180299, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.28560350118, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'right')
Agent attempted driving forward through a red light. (rewarded -9.29)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.4487 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (0, -1), action: None, reward: 2.57592493441
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 17, 't': 8, 'action': None, 'reward': 2.5759249344096453, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 2.57592493441, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.58)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.4602 >= 0.0000
Environment.act() [POST]: location: (1, 4), heading: (-1, 0), action: left, reward: 2.46479027898
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 2.464790278983781, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 2.46479027898, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.46)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.7916 >= 0.0000
Environment.act() [POST]: location: (1, 3), heading: (0, -1), action: right, reward: 1.68519484458
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'right'), 'deadline': 15, 't': 10, 'action': 'right', 'reward': 1.6851948445818843, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -14.85809777752475}, action: right, reward: 1.68519484458, Q[state]-after: {'forward': 0.0, 'right': 1.4084255061661923, None: 0.0, 'left': -14.85809777752475}
Agent previous state: ('right', 'green', None, None, 'right')
Agent followed the waypoint right. (rewarded 1.69)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.5470 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: right, reward: 0.559311679975
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 14, 't': 11, 'action': 'right', 'reward': 0.5593116799751529, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 0.559311679975, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.56)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.5841 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (1, 0), action: forward, reward: -9.64976990484
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'right', 'left'), 'deadline': 13, 't': 12, 'action': 'forward', 'reward': -9.649769904837157, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': -0.23523747748943724, None: 0.0, 'left': -4.646156732249808}, action: forward, reward: -9.64976990484, Q[state]-after: {'forward': 0.0, 'right': -0.23523747748943724, None: 0.0, 'left': -4.646156732249808}
Agent previous state: ('left', 'red', None, 'right', 'left')
Agent attempted driving forward through a red light. (rewarded -9.65)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.7380 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: left, reward: 2.11394509304
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 12, 't': 13, 'action': 'left', 'reward': 2.113945093042018, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: left, reward: 2.11394509304, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 2.11)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.7527 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: right, reward: 0.7206059527
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'forward'), 'deadline': 11, 't': 14, 'action': 'right', 'reward': 0.7206059526995766, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.658791584185761, 'right': 1.1639131091510733, None: 0.0, 'left': 0.0}, action: right, reward: 0.7206059527, Q[state]-after: {'forward': -4.658791584185761, 'right': 1.1639131091510733, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'forward')
Agent drove right instead of left. (rewarded 0.72)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.9629 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: 0.749000247255
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 10, 't': 15, 'action': 'right', 'reward': 0.7490002472545032, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 0.749000247255, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 0.75)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.9232 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: right, reward: 2.04939987537
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 9, 't': 16, 'action': 'right', 'reward': 2.0493998753682785, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}, action: right, reward: 2.04939987537, Q[state]-after: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent followed the waypoint right. (rewarded 2.05)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.4247 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: 0.95363801944
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, 'forward', None), 'deadline': 8, 't': 17, 'action': None, 'reward': 0.9536380194404754, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.1629290961929437, 'left': 0.0}, action: None, reward: 0.95363801944, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.1629290961929437, 'left': 0.0}
Agent previous state: ('forward', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 0.95)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.8081 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: 0.634185820992
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 7, 't': 18, 'action': None, 'reward': 0.634185820991553, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 0.634185820992, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.63)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.7801 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: right, reward: 0.884675622224
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', None), 'deadline': 6, 't': 19, 'action': 'right', 'reward': 0.8846756222240263, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.9134802972191687, None: 0.0, 'left': 0.16631087142739243}, action: right, reward: 0.884675622224, Q[state]-after: {'forward': 0.0, 'right': 0.9134802972191687, None: 0.0, 'left': 0.16631087142739243}
Agent previous state: ('forward', 'green', None, 'forward', None)
Agent drove right instead of forward. (rewarded 0.88)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Environment.step(): t = 20
Agent selected maxQ 0.9937 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: right, reward: 0.220848325557
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'forward'), 'deadline': 5, 't': 20, 'action': 'right', 'reward': 0.22084832555748712, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.05966342424560073, None: 0.0, 'left': 0.0}, action: right, reward: 0.220848325557, Q[state]-after: {'forward': 0.0, 'right': 0.05966342424560073, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'right', None, 'forward')
Agent drove right instead of left. (rewarded 0.22)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Environment.step(): t = 21
Agent selected maxQ 0.4756 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: None, reward: 0.535065443845
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 4, 't': 21, 'action': None, 'reward': 0.5350654438445313, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}, action: None, reward: 0.535065443845, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.54)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Environment.step(): t = 22
Agent selected maxQ 0.5103 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: 1.54018005207
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, None), 'deadline': 3, 't': 22, 'action': 'right', 'reward': 1.5401800520666016, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, None), Q[state]-before: {'forward': -4.682235054855685, 'right': 1.2753355391734351, None: 0.0, 'left': -5.44236463690834}, action: right, reward: 1.54018005207, Q[state]-after: {'forward': -4.682235054855685, 'right': 1.2753355391734351, None: 0.0, 'left': -5.44236463690834}
Agent previous state: ('right', 'red', 'right', None, None)
Agent followed the waypoint right. (rewarded 1.54)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Environment.step(): t = 23
Agent selected maxQ 0.7183 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: None, reward: 0.792367683669
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 2, 't': 23, 'action': None, 'reward': 0.7923676836692726, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}, action: None, reward: 0.792367683669, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.79)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Environment.step(): t = 24
Agent selected maxQ 0.5654 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: right, reward: 0.352068612493
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'left'), 'deadline': 1, 't': 24, 'action': 'right', 'reward': 0.3520686124928889, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.089686061455693, None: 0.0, 'left': 0.0}, action: right, reward: 0.352068612493, Q[state]-after: {'forward': 0.0, 'right': 1.089686061455693, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', 'right', None, 'left')
Agent followed the waypoint right. (rewarded 0.35)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 7
\-------------------------

Environment.reset(): Trial set up with start = (2, 7), destination = (7, 2), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.1160 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: -40.817773484
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'left', None), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -40.81777348402552, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.984022710954665}, action: forward, reward: -40.817773484, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -19.984022710954665}
Agent previous state: ('right', 'red', 'forward', 'left', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.82)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.8967 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: -9.89280501608
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', 'left', None), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': -9.892805016083063, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.89280501608, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', 'left', None)
Agent attempted driving forward through a red light. (rewarded -9.89)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.8603 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (1, 0), action: forward, reward: -10.2611902036
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', None, 'left', 'right'), 'deadline': 18, 't': 2, 'action': 'forward', 'reward': -10.261190203632463, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.2611902036, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', 'right')
Agent attempted driving forward through a red light. (rewarded -10.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.6062 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, 1), action: right, reward: 2.14661948631
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 17, 't': 3, 'action': 'right', 'reward': 2.1466194863086017, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 2.14661948631, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.15)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.2423 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (1, 0), action: left, reward: 1.51288917079
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 16, 't': 4, 'action': 'left', 'reward': 1.5128891707883914, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}, action: left, reward: 1.51288917079, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.9908 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: right, reward: 2.29099932965
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'forward', None), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 2.290999329648918, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}, action: right, reward: 2.29099932965, Q[state]-after: {'forward': 0.0, 'right': 1.0608504350824939, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'forward', None)
Agent followed the waypoint right. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.8364 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: None, reward: 0.953582506539
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', None), 'deadline': 14, 't': 6, 'action': None, 'reward': 0.9535825065393957, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.7533937449098496, 'left': 0.0}, action: None, reward: 0.953582506539, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.7533937449098496, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', None)
Agent properly idled at a red light. (rewarded 0.95)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.1859 >= 0.0000
Environment.act() [POST]: location: (3, 4), heading: (0, 1), action: forward, reward: 0.116610292649
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', None), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 0.11661029264869516, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', None), Q[state]-before: {'forward': 0.7593867387977282, 'right': 0.0, None: 0.0, 'left': 0.4758442289934988}, action: forward, reward: 0.116610292649, Q[state]-after: {'forward': 0.7593867387977282, 'right': 0.0, None: 0.0, 'left': 0.4758442289934988}
Agent previous state: ('right', 'green', None, 'left', None)
Agent drove forward instead of right. (rewarded 0.12)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.8376 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: right, reward: 1.07460160232
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.0746016023163112, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 1.07460160232, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 1.07)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.6715 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: None, reward: 1.91452261586
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.914522615858705, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 1.91452261586, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.5912 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: -40.5143732207
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'forward', None, 'left'), 'deadline': 10, 't': 10, 'action': 'forward', 'reward': -40.514373220666045, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'forward', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.5143732207, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'forward', None, 'left')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.51)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.7304 >= 0.0000
Environment.act() [POST]: location: (2, 4), heading: (-1, 0), action: forward, reward: -10.4944599038
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'left'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'left', None, 'left'), 'deadline': 9, 't': 11, 'action': 'forward', 'reward': -10.494459903836253, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.4944599038, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.49)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.1630 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: right, reward: 0.360985640877
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 8, 't': 12, 'action': 'right', 'reward': 0.3609856408773817, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 0.360985640877, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 0.36)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.7578 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: -9.68099012322
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': None}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', None, 'left', 'forward'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': -9.680990123223538, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -6.87511476262581}, action: forward, reward: -9.68099012322, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -6.87511476262581}
Agent previous state: ('left', 'red', None, 'left', 'forward')
Agent attempted driving forward through a red light. (rewarded -9.68)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.6865 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: -10.2870067681
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'left', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('left', 'red', 'right', 'left', 'forward'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': -10.287006768109034, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'right', 'left', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.2870067681, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'right', 'left', 'forward')
Agent attempted driving forward through a red light. (rewarded -10.29)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.2483 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (0, -1), action: forward, reward: -40.0434502543
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('left', 'red', 'forward', None, 'forward'), 'deadline': 5, 't': 15, 'action': 'forward', 'reward': -40.0434502542938, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.0434502543, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', None, 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.04)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.3965 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: forward, reward: -0.18829542345
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'left', 'forward', 'forward'), 'deadline': 4, 't': 16, 'action': 'forward', 'reward': -0.1882954234497709, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'left', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -0.18829542345, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', 'left', 'forward', 'forward')
Agent drove forward instead of left. (rewarded -0.19)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.7456 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: 2.05500514997
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', None, None), 'deadline': 3, 't': 17, 'action': None, 'reward': 2.055005149972391, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 1.2188229928147303, 'left': -7.124417456295234}, action: None, reward: 2.05500514997, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 1.2188229928147303, 'left': -7.124417456295234}
Agent previous state: ('left', 'red', 'left', None, None)
Agent properly idled at a red light. (rewarded 2.06)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.8329 >= 0.0000
Environment.act() [POST]: location: (2, 2), heading: (0, -1), action: None, reward: 1.93979762014
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 2, 't': 18, 'action': None, 'reward': 1.939797620144406, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.2752380229939957, None: 1.3356536212394092, 'left': 0.0}, action: None, reward: 1.93979762014, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.2752380229939957, None: 1.3356536212394092, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.94)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.5618 >= 0.0000
Environment.act() [POST]: location: (2, 7), heading: (0, -1), action: forward, reward: 0.0716906720985
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', None, 'left'), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.07169067209848456, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.0716906720985, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'right', None, 'left')
Agent drove forward instead of left. (rewarded 0.07)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 8
\-------------------------

Environment.reset(): Trial set up with start = (7, 2), destination = (4, 3), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.1954 >= 0.0000
Environment.act() [POST]: location: (7, 2), heading: (0, 1), action: forward, reward: -40.2041338492
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'left', 'forward', 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': -40.2041338492456, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -40.2041338492, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.2847 >= 0.0000
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: right, reward: 1.7308115676
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'right'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'right', None, 'forward'), 'deadline': 19, 't': 1, 'action': 'right', 'reward': 1.7308115675955438, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.8357088649250979, None: 0.0, 'left': 0.0}, action: right, reward: 1.7308115676, Q[state]-after: {'forward': 0.0, 'right': 0.8357088649250979, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', None, 'forward')
Agent followed the waypoint right. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.0418 >= 0.0000
Environment.act() [POST]: location: (6, 2), heading: (-1, 0), action: None, reward: 2.68780301582
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, None), 'deadline': 18, 't': 2, 'action': None, 'reward': 2.687803015822176, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, None), Q[state]-before: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}, action: None, reward: 2.68780301582, Q[state]-after: {'forward': -7.446278756312308, 'right': 0.30206218445040467, None: 2.245907541008686, 'left': -9.115363129092575}
Agent previous state: ('forward', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.3461 >= 0.0000
Environment.act() [POST]: location: (5, 2), heading: (-1, 0), action: forward, reward: 2.0123612838
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'left', None, None), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 2.0123612838047844, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'left', None, None), Q[state]-before: {'forward': 1.1029277368284158, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}, action: forward, reward: 2.0123612838, Q[state]-after: {'forward': 1.1029277368284158, 'right': 0.0, None: -2.487799446009724, 'left': 0.0}
Agent previous state: ('forward', 'green', 'left', None, None)
Agent followed the waypoint forward. (rewarded 2.01)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.2080 >= 0.0000
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: right, reward: 1.17533825898
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 16, 't': 4, 'action': 'right', 'reward': 1.175338258978932, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 1.17533825898, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.18)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.7305 >= 0.0000
Environment.act() [POST]: location: (4, 7), heading: (-1, 0), action: left, reward: 1.83429498707
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 15, 't': 5, 'action': 'left', 'reward': 1.8342949870703829, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 1.83429498707, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.6747 >= 0.0000
Environment.act() [POST]: location: (4, 2), heading: (0, 1), action: left, reward: 2.30284594657
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 6, 'action': 'left', 'reward': 2.302845946569746, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 2.30284594657, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.30)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.0973 >= 0.0000
Environment.act() [POST]: location: (4, 2), heading: (0, 1), action: forward, reward: -10.1242296612
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('forward', 'red', 'right', None, 'left'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': -10.124229661151144, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'right', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -10.1242296612, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'right', None, 'left')
Agent attempted driving forward through a red light. (rewarded -10.12)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.0591 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (-1, 0), action: right, reward: 1.29698560457
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'left'), 'deadline': 12, 't': 8, 'action': 'right', 'reward': 1.296985604568333, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}, action: right, reward: 1.29698560457, Q[state]-after: {'forward': 0.0, 'right': 1.100850293357699, None: 0.0, 'left': -7.5241302672277275}
Agent previous state: ('forward', 'red', None, None, 'left')
Agent drove right instead of forward. (rewarded 1.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.0408 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: left, reward: 1.1341821166
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 11, 't': 9, 'action': 'left', 'reward': 1.134182116603668, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 1.1341821166, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.13)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.1933 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: right, reward: -0.0743658793449
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 10, 't': 10, 'action': 'right', 'reward': -0.07436587934490013, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}, action: right, reward: -0.0743658793449, Q[state]-after: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent drove right instead of left. (rewarded -0.07)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.2406 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: None, reward: 1.55489823291
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 9, 't': 11, 'action': None, 'reward': 1.5548982329117353, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}, action: None, reward: 1.55489823291, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.7444 >= 0.0000
Environment.act() [POST]: location: (2, 3), heading: (-1, 0), action: forward, reward: -9.27715589871
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': 'right', 'left': 'right'}, 'violation': 2, 'light': 'red', 'state': ('right', 'red', 'right', 'right', 'left'), 'deadline': 8, 't': 12, 'action': 'forward', 'reward': -9.277155898706573, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'right', 'right', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -9.27715589871, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'right', 'right', 'left')
Agent attempted driving forward through a red light. (rewarded -9.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.4948 >= 0.0000
Environment.act() [POST]: location: (1, 3), heading: (-1, 0), action: forward, reward: 0.844064851124
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, 'left', 'left'), 'deadline': 7, 't': 13, 'action': 'forward', 'reward': 0.8440648511244421, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, 'left', 'left'), Q[state]-before: {'forward': 0.552973198228982, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.844064851124, Q[state]-after: {'forward': 0.552973198228982, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', None, 'left', 'left')
Agent drove forward instead of right. (rewarded 0.84)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.2645 >= 0.0000
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: right, reward: 0.791237345154
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'right', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'left', 'right', None), 'deadline': 6, 't': 14, 'action': 'right', 'reward': 0.7912373451541614, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'left', 'right', None), Q[state]-before: {'forward': -5.440408721147249, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: 0.791237345154, Q[state]-after: {'forward': -5.440408721147249, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'left', 'right', None)
Agent followed the waypoint right. (rewarded 0.79)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.1303 >= 0.0000
Environment.act() [POST]: location: (1, 2), heading: (0, -1), action: None, reward: 0.663144343703
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, None), 'deadline': 5, 't': 15, 'action': None, 'reward': 0.6631443437026916, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, None), Q[state]-before: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}, action: None, reward: 0.663144343703, Q[state]-after: {'forward': -8.881999212504784, 'right': 1.4732944185955201, None: 1.8994628294058953, 'left': -9.967344384190643}
Agent previous state: ('right', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 0.66)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.9592 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: left, reward: 1.20055518316
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'left'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'left', None, None), 'deadline': 4, 't': 16, 'action': 'left', 'reward': 1.200555183157927, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'left', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}, action: left, reward: 1.20055518316, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -4.72007984434569, 'left': 0.3688534006025197}
Agent previous state: ('right', 'green', 'left', None, None)
Agent drove left instead of right. (rewarded 1.20)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.5254 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: -0.60916275431
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', 'left', 'forward', None), 'deadline': 3, 't': 17, 'action': 'right', 'reward': -0.6091627543095833, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'left', 'forward', None), Q[state]-before: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}, action: right, reward: -0.60916275431, Q[state]-after: {'forward': -20.271210455197465, 'right': 0.0, None: 0.0, 'left': -20.146306728683804}
Agent previous state: ('left', 'red', 'left', 'forward', None)
Agent drove right instead of left. (rewarded -0.61)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.7362 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: None, reward: 0.487316399363
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', 'forward', None, 'forward'), 'deadline': 2, 't': 18, 'action': None, 'reward': 0.4873163993634342, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', None, 'forward'), Q[state]-before: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}, action: None, reward: 0.487316399363, Q[state]-after: {'forward': -19.99594693500603, 'right': -10.145439338589624, None: 1.6548736244572657, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', None, 'forward')
Agent properly idled at a red light. (rewarded 0.49)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.2969 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: None, reward: 0.925811754562
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'forward'), 'deadline': 1, 't': 19, 'action': None, 'reward': 0.9258117545619249, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'forward'), Q[state]-before: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 2.2894242512020195, 'left': 0.0}, action: None, reward: 0.925811754562, Q[state]-after: {'forward': -4.895273665926438, 'right': 1.5094000576471682, None: 2.2894242512020195, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('right', 'red', None, None, 'forward')
Agent properly idled at a red light. (rewarded 0.93)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 9
\-------------------------

Environment.reset(): Trial set up with start = (3, 7), destination = (6, 3), deadline = 25

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.0178 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: right, reward: -19.003255635
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'forward'}, 'violation': 3, 'light': 'red', 'state': ('left', 'red', 'forward', 'forward', None), 'deadline': 25, 't': 0, 'action': 'right', 'reward': -19.003255635030698, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', 'forward', 'forward', None), Q[state]-before: {'forward': -19.928152071331827, 'right': 0.0, None: 0.0, 'left': 0.0}, action: right, reward: -19.003255635, Q[state]-after: {'forward': -19.928152071331827, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'red', 'forward', 'forward', None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.00)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.7447 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: None, reward: 1.13625554503
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 24, 't': 1, 'action': None, 'reward': 1.1362555450320493, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.13625554503, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.9600 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: None, reward: 1.84025445817
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, 'forward', None), 'deadline': 23, 't': 2, 'action': None, 'reward': 1.8402544581746096, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, 'forward', None), Q[state]-before: {'forward': -19.546606033953985, 'right': 0.0, None: 1.2658942648054654, 'left': 0.0}, action: None, reward: 1.84025445817, Q[state]-after: {'forward': -19.546606033953985, 'right': 0.0, None: 1.2658942648054654, 'left': 0.0}
Agent previous state: ('left', 'red', None, 'forward', None)
Agent properly idled at a red light. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.4444 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: None, reward: 1.89919081909
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, None), 'deadline': 22, 't': 3, 'action': None, 'reward': 1.899190819090655, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, None), Q[state]-before: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}, action: None, reward: 1.89919081909, Q[state]-after: {'forward': -7.399836502402955, 'right': 0.775451469938683, None: 1.5792756264959948, 'left': -9.153640767014467}
Agent previous state: ('left', 'red', None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.6897 >= 0.0000
Environment.act() [POST]: location: (3, 7), heading: (-1, 0), action: None, reward: 1.13638153034
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('left', 'red', None, None, 'left'), 'deadline': 21, 't': 4, 'action': None, 'reward': 1.1363815303445937, 'waypoint': 'left'}
Agent.update() - state: ('left', 'red', None, None, 'left'), Q[state]-before: {'forward': -7.756531448092278, 'right': 1.2752380229939957, None: 1.3356536212394092, 'left': 0.0}, action: None, reward: 1.13638153034, Q[state]-after: {'forward': -7.756531448092278, 'right': 1.2752380229939957, None: 1.3356536212394092, 'left': 0.0}
Agent previous state: ('left', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.8094 >= 0.0000
Environment.act() [POST]: location: (3, 2), heading: (0, 1), action: left, reward: 1.21674225508
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 20, 't': 5, 'action': 'left', 'reward': 1.216742255083852, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: left, reward: 1.21674225508, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.8077 >= 0.0000
Environment.act() [POST]: location: (3, 3), heading: (0, 1), action: forward, reward: 1.31348864168
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, 'left', 'right'), 'deadline': 19, 't': 6, 'action': 'forward', 'reward': 1.3134886416829894, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, 'left', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.31348864168, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('left', 'green', None, 'left', 'right')
Agent drove forward instead of left. (rewarded 1.31)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.7734 >= 0.0000
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: left, reward: 2.28798372766
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 18, 't': 7, 'action': 'left', 'reward': 2.2879837276639567, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 2.28798372766, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 2.29)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.4155 >= 0.0000
Environment.act() [POST]: location: (4, 4), heading: (0, 1), action: right, reward: 1.59574567297
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', 'forward', None, None), 'deadline': 17, 't': 8, 'action': 'right', 'reward': 1.5957456729704966, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', 'forward', None, None), Q[state]-before: {'forward': 0.0, 'right': 0.7802351202602501, None: 0.0, 'left': 0.0}, action: right, reward: 1.59574567297, Q[state]-after: {'forward': 0.0, 'right': 0.7802351202602501, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', 'forward', None, None)
Agent drove right instead of forward. (rewarded 1.60)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.1954 >= 0.0000
Environment.act() [POST]: location: (5, 4), heading: (1, 0), action: left, reward: 1.41266075059
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, 'left'), 'deadline': 16, 't': 9, 'action': 'left', 'reward': 1.4126607505917497, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}, action: left, reward: 1.41266075059, Q[state]-after: {'forward': 0.04020354638699758, 'right': 0.0, None: 0.517006618778459, 'left': 2.343331058514302}
Agent previous state: ('left', 'green', None, None, 'left')
Agent followed the waypoint left. (rewarded 1.41)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.0502 >= 0.0000
Environment.act() [POST]: location: (6, 4), heading: (1, 0), action: forward, reward: 1.80038463305
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'left'), 'deadline': 15, 't': 10, 'action': 'forward', 'reward': 1.800384633053561, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.80038463305, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'forward', 'left')
Agent followed the waypoint forward. (rewarded 1.80)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.0578 >= 0.0000
Environment.act(): Primary agent has reached destination!
Environment.act() [POST]: location: (6, 3), heading: (0, -1), action: left, reward: 1.83807910533
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', None, None, None), 'deadline': 14, 't': 11, 'action': 'left', 'reward': 1.8380791053250145, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', None, None, None), Q[state]-before: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}, action: left, reward: 1.83807910533, Q[state]-after: {'forward': 0.27498081749571857, 'right': 0.4183196233299713, None: -4.081925127081703, 'left': 1.5256115807930413}
Agent previous state: ('left', 'green', None, None, None)
Agent followed the waypoint left. (rewarded 1.84)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 10
\-------------------------

Environment.reset(): Trial set up with start = (3, 3), destination = (6, 2), deadline = 20

/-------------------
| Step 0 Results
\-------------------

Environment.step(): t = 0
Agent selected maxQ 0.4197 >= 0.0000
Environment.act() [POST]: location: (4, 3), heading: (1, 0), action: forward, reward: 2.39219893117
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'right', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'right', 'forward'), 'deadline': 20, 't': 0, 'action': 'forward', 'reward': 2.3921989311703262, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'right', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.39219893117, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'right', 'forward')
Agent followed the waypoint forward. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Environment.step(): t = 1
Agent selected maxQ 0.9782 >= 0.0000
Environment.act() [POST]: location: (5, 3), heading: (1, 0), action: forward, reward: 2.80027362755
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'forward', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'forward', 'right'), 'deadline': 19, 't': 1, 'action': 'forward', 'reward': 2.800273627545563, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'forward', 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 2.80027362755, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'green', None, 'forward', 'right')
Agent followed the waypoint forward. (rewarded 2.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Environment.step(): t = 2
Agent selected maxQ 0.5529 >= 0.0000
Environment.act() [POST]: location: (5, 2), heading: (0, -1), action: left, reward: 1.25016144058
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'green', 'state': ('forward', 'green', None, 'left', None), 'deadline': 18, 't': 2, 'action': 'left', 'reward': 1.2501614405774117, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'green', None, 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}, action: left, reward: 1.25016144058, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.9667714568663331}
Agent previous state: ('forward', 'green', None, 'left', None)
Agent drove left instead of forward. (rewarded 1.25)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Environment.step(): t = 3
Agent selected maxQ 0.4017 >= 0.0000
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: 1.28056211321
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': 'left', 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', 'left', 'right'), 'deadline': 17, 't': 3, 'action': 'forward', 'reward': 1.2805621132133678, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', 'left', 'right'), Q[state]-before: {'forward': 0.23585538278145152, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.28056211321, Q[state]-after: {'forward': 0.23585538278145152, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', 'left', 'right')
Agent drove forward instead of right. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Environment.step(): t = 4
Agent selected maxQ 0.8707 >= 0.0000
Environment.act() [POST]: location: (5, 7), heading: (0, -1), action: forward, reward: -39.4377041709
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': 'forward', 'left': 'forward'}, 'violation': 4, 'light': 'red', 'state': ('right', 'red', 'forward', 'forward', 'forward'), 'deadline': 16, 't': 4, 'action': 'forward', 'reward': -39.43770417089302, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', 'forward', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.4377041709, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', 'forward', 'forward', 'forward')
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.44)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Environment.step(): t = 5
Agent selected maxQ 0.1565 >= 0.0000
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: right, reward: 2.78685104641
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'forward'), 'deadline': 15, 't': 5, 'action': 'right', 'reward': 2.7868510464071736, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'forward'), Q[state]-before: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}, action: right, reward: 2.78685104641, Q[state]-after: {'forward': 0.0, 'right': 1.562243329909458, None: -3.8737587374268805, 'left': -10.250437288114005}
Agent previous state: ('right', 'green', None, None, 'forward')
Agent followed the waypoint right. (rewarded 2.79)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Environment.step(): t = 6
Agent selected maxQ 0.9985 >= 0.0000
Environment.act() [POST]: location: (6, 7), heading: (1, 0), action: None, reward: 0.0103421679364
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 14, 't': 6, 'action': None, 'reward': 0.010342167936393087, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 0.0103421679364, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Environment.step(): t = 7
Agent selected maxQ 0.6053 >= 0.0000
Environment.act() [POST]: location: (7, 7), heading: (1, 0), action: forward, reward: 0.665306508517
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'left'), 'deadline': 13, 't': 7, 'action': 'forward', 'reward': 0.6653065085174714, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 0.665306508517, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', None, 'left')
Agent drove forward instead of right. (rewarded 0.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Environment.step(): t = 8
Agent selected maxQ 0.7152 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: forward, reward: 1.75419308314
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'right', 'right': None, 'left': 'forward'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'forward', None, 'right'), 'deadline': 12, 't': 8, 'action': 'forward', 'reward': 1.754193083140053, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'forward', None, 'right'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: 1.75419308314, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'green', 'forward', None, 'right')
Agent drove forward instead of right. (rewarded 1.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Environment.step(): t = 9
Agent selected maxQ 0.5326 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: 1.84264541709
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 11, 't': 9, 'action': None, 'reward': 1.8426454170926017, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}, action: None, reward: 1.84264541709, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 1.84)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Environment.step(): t = 10
Agent selected maxQ 0.1101 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: 2.09154267118
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, None, 'left'), 'deadline': 10, 't': 10, 'action': None, 'reward': 2.091542671184517, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, None, 'left'), Q[state]-before: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}, action: None, reward: 2.09154267118, Q[state]-after: {'forward': -8.041042883195976, 'right': 0.8051758379624158, None: 2.372298883293033, 'left': 0.0}
Agent previous state: ('right', 'red', None, None, 'left')
Agent properly idled at a red light. (rewarded 2.09)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Environment.step(): t = 11
Agent selected maxQ 0.1659 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: 0.429318434666
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 9, 't': 11, 'action': None, 'reward': 0.429318434665926, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 0.429318434666, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 0.43)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Environment.step(): t = 12
Agent selected maxQ 0.1493 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: 1.52280371365
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 8, 't': 12, 'action': None, 'reward': 1.5228037136528925, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 1.52280371365, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.52)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Environment.step(): t = 13
Agent selected maxQ 0.2363 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (1, 0), action: None, reward: 1.00702442672
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'left', 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, 'left'), 'deadline': 7, 't': 13, 'action': None, 'reward': 1.0070244267189477, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, 'left'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}, action: None, reward: 1.00702442672, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.5186409882611736, 'left': -0.09759033182575594}
Agent previous state: ('right', 'green', None, None, 'left')
Agent idled at a green light with oncoming traffic. (rewarded 1.01)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Environment.step(): t = 14
Agent selected maxQ 0.9448 >= 0.0000
Environment.act() [POST]: location: (1, 7), heading: (1, 0), action: forward, reward: 0.984374605296
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': 'forward', 'right': 'forward', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', 'right', 'forward', 'forward'), 'deadline': 6, 't': 14, 'action': 'forward', 'reward': 0.984374605295902, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', 'right', 'forward', 'forward'), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.082732849774809}, action: forward, reward: 0.984374605296, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': -10.082732849774809}
Agent previous state: ('right', 'green', 'right', 'forward', 'forward')
Agent drove forward instead of right. (rewarded 0.98)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Environment.step(): t = 15
Agent selected maxQ 0.9395 >= 0.0000
Environment.act() [POST]: location: (1, 2), heading: (0, 1), action: right, reward: 2.40472891314
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': None, 'left': None}, 'violation': 0, 'light': 'green', 'state': ('right', 'green', None, None, None), 'deadline': 5, 't': 15, 'action': 'right', 'reward': 2.4047289131403584, 'waypoint': 'right'}
Agent.update() - state: ('right', 'green', None, None, None), Q[state]-before: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}, action: right, reward: 2.40472891314, Q[state]-after: {'forward': 0.22883331673999138, 'right': 1.7899284987503719, None: -2.8282507371677634, 'left': 0.5452371318815935}
Agent previous state: ('right', 'green', None, None, None)
Agent followed the waypoint right. (rewarded 2.40)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Environment.step(): t = 16
Agent selected maxQ 0.5324 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: right, reward: 1.39769679265
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'left', 'left': None}, 'violation': 0, 'light': 'red', 'state': ('right', 'red', None, 'left', None), 'deadline': 4, 't': 16, 'action': 'right', 'reward': 1.397696792650357, 'waypoint': 'right'}
Agent.update() - state: ('right', 'red', None, 'left', None), Q[state]-before: {'forward': -7.369319515974606, 'right': 1.7416606689342777, None: 0.0, 'left': 0.0}, action: right, reward: 1.39769679265, Q[state]-after: {'forward': -7.369319515974606, 'right': 1.7416606689342777, None: 0.0, 'left': 0.0}
Agent previous state: ('right', 'red', None, 'left', None)
Agent followed the waypoint right. (rewarded 1.40)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Environment.step(): t = 17
Agent selected maxQ 0.6010 >= 0.0000
Environment.act() [POST]: location: (8, 2), heading: (-1, 0), action: forward, reward: -39.2479216486
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': None, 'right': 'forward', 'left': 'left'}, 'violation': 4, 'light': 'red', 'state': ('forward', 'red', 'left', 'forward', None), 'deadline': 3, 't': 17, 'action': 'forward', 'reward': -39.247921648590186, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', 'left', 'forward', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}, action: forward, reward: -39.2479216486, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', 'left', 'forward', None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.25)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Environment.step(): t = 18
Agent selected maxQ 0.7190 >= 0.0000
Environment.act() [POST]: location: (8, 7), heading: (0, -1), action: right, reward: 0.287241590383
Environment.act(): Step data: {'inputs': {'light': 'red', 'oncoming': 'forward', 'right': None, 'left': None}, 'violation': 0, 'light': 'red', 'state': ('forward', 'red', None, None, 'forward'), 'deadline': 2, 't': 18, 'action': 'right', 'reward': 0.2872415903829719, 'waypoint': 'forward'}
Agent.update() - state: ('forward', 'red', None, None, 'forward'), Q[state]-before: {'forward': -7.669334934958838, 'right': 0.36188645308689993, None: 0.0, 'left': 0.0}, action: right, reward: 0.287241590383, Q[state]-after: {'forward': -7.669334934958838, 'right': 0.36188645308689993, None: 0.0, 'left': 0.0}
Agent previous state: ('forward', 'red', None, None, 'forward')
Agent drove right instead of forward. (rewarded 0.29)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Environment.step(): t = 19
Agent selected maxQ 0.2443 >= 0.0000
Environment.act() [POST]: location: (8, 6), heading: (0, -1), action: forward, reward: 0.0716019100717
Environment.act(): Step data: {'inputs': {'light': 'green', 'oncoming': None, 'right': 'left', 'left': 'right'}, 'violation': 0, 'light': 'green', 'state': ('left', 'green', 'right', 'left', None), 'deadline': 1, 't': 19, 'action': 'forward', 'reward': 0.07160191007174743, 'waypoint': 'left'}
Agent.update() - state: ('left', 'green', 'right', 'left', None), Q[state]-before: {'forward': 0.0, 'right': 0.0, None: -2.8331702077724357, 'left': 0.0}, action: forward, reward: 0.0716019100717, Q[state]-after: {'forward': 0.0, 'right': 0.0, None: -2.8331702077724357, 'left': 0.0}
Environment.step(): Primary agent ran out of time! Trial aborted.
Agent previous state: ('left', 'green', 'right', 'left', None)
Agent drove forward instead of left. (rewarded 0.07)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

Simulation ended. . . 
